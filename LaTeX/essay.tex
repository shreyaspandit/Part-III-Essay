% Template for Part III essays 2024/25 (12.02.2025)
% Authors: Mycroft Rosca-Mead, Matt Colbrook, Jonathan Evans.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% The following few lines are preliminaries and MUST NOT BE EDITED
\documentclass[11pt, titlepage]{article} % DO NOT CHANGE THE FONT SIZE
\usepackage[left=2.2cm,right=2.2cm,top=2.5cm,bottom=2.5cm]{geometry} % DO NOT CHANGE THE MARGINS
\usepackage{amsfonts,amssymb,amsthm,amsmath,amscd}
\usepackage{enumerate}
\usepackage{tikz}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% ESSAY TITLE
% Your essay title should match the title given in the essay booklet.
% If you wish to give the essay your own subtitle, you can do this after the title page (e.g. using \section*{Subtitle Name}).
% You do not need to use \Author and YOU SHOULD NOT INCLUDE YOUR NAME OR COLLEGE ANYWHERE IN THIS ESSAY.
% The title page does not count towards the total page number.
\title{New Advances in Conformal Prediction}

% DATE
%To remove the date from the front page of the final version, UNCOMMENT the line below.
%\date{}

% FONTS
%\renewcommand{\familydefault}{\sfdefault}
% YOU MUST USE EITHER THE SANS-SERIF FONT DEFINED BY THE COMMAND ABOVE OR THE STANDARD LaTeX (computer modern) FONT THAT CAN BE OBTAINED BY COMMENTING OUT THE LINE ABOVE. YOU MUST NOT USE ANY OTHER FONT.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% A guide to using LaTeX can be found online at https://www.overleaf.com/learn/latex/Learn_LaTeX_in_30_minutes.
% A shorter LaTeX cheat sheet can be found at https://wch.github.io/latexsheet/.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Some latex tips:

% Use this space for any special macros or packages you need.
\usepackage{graphicx}
\usepackage[backend=biber, sorting=none, style=alphabetic]{biblatex}
\addbibresource{refs.bib}

\usepackage{bbm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{caption}
\usepackage{subcaption}

\DeclareMathOperator*{\argmin}{argmin}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\newcommand{\R}{\mathrm}
\newcommand{\B}{\mathbf}
\newcommand{\Prob}[1]{\mathbb{P}\left( #1 \right)}
\newcommand{\Exp}[3]{\mathbb{E}\left#2 #1 \right#3}
\newcommand{\Ind}[1]{\mathbbm{1}\left\{ #1 \right\}}

\usepackage{hyperref}
\usepackage{mathtools}
% use this package for easy referencing:
\usepackage[capitalise,noabbrev]{cleveref} % e.g., \cref{amazing_theorem} will reference "`Theorem"' X without having to keep track of theorems, equations etc and typing "`Theorem"'
\usepackage{overpic} % use this package for easy labelling of figures
\usepackage{url} % use this package to reference urls, e.g., \url{https://www.maths.cam.ac.uk/postgrad/part-iii/prospective.html}
\usepackage{comment} % use this package to comment things out \begin{comment} blah \end{comment}

% FIGURES
% Here is an example of how to do figures:

%\begin{figure}
%\centering
%\includegraphics[width=\linewidth]{CMS at night_0.jpg}
%\caption{The CMS at night.}\label{fig:cms}
%\end{figure}

% General tip: Many tex problems can be easily solved by a quick and specific google search.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%FOOTNOTES
\newcommand\blfootnote[1]{% avoids footnotes spilling over
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% The following relate to equations, theorems, etc, as illustarted in the document. 

\numberwithin{equation}{section}
% EQUATION NUMBERING: The command above can be changed to number equations within subsections, rather than sections, or it can be commented out to number equations through the document, irrespective of (sub)sections.

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}

\numberwithin{theorem}{section}
\numberwithin{lemma}{section}
\numberwithin{corollary}{section}
\numberwithin{proposition}{section}
\numberwithin{definition}{section}
\numberwithin{remark}{section}
% NUMBERING OF THEOREMS ETC: The commands above can be changed to number within subsections, rather than sections, or commented out to number them through the document, irrespective of (sub)sections.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% START WRITING YOUR ESSAY HERE

\section{Introduction}

Quantifying the uncertainty in a prediction produced by a regression or classification model is important to enable these models to be confidently deployed in real-world settings. This typically means that a procedure outputs a set of predictions, as opposed to a single prediction, which contains the true value of the response with a high probability. For example, it is well-known that one can construct exact prediction intervals in the normal linear model and asymptotically valid prediction intervals for generalised linear models. In Bayesian statistics, one can calculate the posterior predictive distribution to quantify uncertainty. An important limitation of all of these methods is that they make strong distributional assumptions on the data-generating process and in some cases, only provide asymptotic guarantees. \vskip5pt

\noindent
Conformal prediction is a framework for uncertainty quantification developed by \cite{vovk2005algorithmic} that constructs prediction sets whose validity holds in finite-samples and does not depend on the exact distribution of the data. The appeal of conformal prediction is that it can be combined with any existing predictive model to generate a valid prediction interval. In particular, we need not make any assumptions on this predictive model. Due to this, and the continuing success of modern machine learning methods at predictive tasks, conformal prediction is becoming an increasingly prominent topic of research in both statistics and machine learning. \vskip5pt

\noindent
The task we will focus on in this essay is as follows. Suppose we are given predictor-response pairs \((X_1, Y_1), \ldots, (X_n, Y_n)\), and we would like to predict the true response \(Y_{n+1}\) corresponding to a new predictor \(X_{n+1}\). We aim to use \((X_1, Y_1), \ldots, (X_n, Y_n)\) to construct a prediction set \(C(X_{n+1})\) such that \[\Prob{Y_{n+1} \in C(X_{n+1})} \geq 1-\alpha, \] where \(\alpha \in (0,1) \) is a chosen confidence level. A result of the above form is referred to as a \textit{coverage guarantee}  \vskip5pt 

\noindent
We begin \cref{sec:foundations_of_CP} by introducing the key mathematical components of conformal prediction. In \cref{subsec:quantiles_and_exchangeability}, we will introduce the notion of exchangeability, which underpins the validity of conformal prediction. \cref{subsec:fullconformal} will detail the \textit{full conformal prediction} procedure and prove that it achieves the above coverage guarantee. In \cref{subsec:splitconformal}, we will study the \textit{split conformal prediction} procedure, which is a more computationally efficient version of conformal prediction, and prove that it also achieves the above coverage guarantee. To conclude \cref{sec:foundations_of_CP}, the aim of \cref{subsec:conformityscore} is to examine specific instances of split conformal prediction that improve the empirical performance of the procedure and demonstrate this with numerical experiments. \vskip5pt

\noindent
In \cref{sec:extensions_of_CP}, we explore extensions to the standard conformal prediction framework presented in \cref{sec:foundations_of_CP}. Whilst exchangeability was fundamental in achieving the coverage guarantees in \cref{sec:foundations_of_CP}, the method presented in \cref{subsec:nex_CP} provides a modified coverage guarantee under violations of exchangeability. A specific kind of violation of exchangeability is distribution shift, and a variant of conformal prediction designed to account for this is presented in \cref{subsec:dist_shift}.

\subsection{Notation}

Throughout the essay, we use the following notation.  \vskip5pt

\noindent
For any positive integer \(n\), we define \([n] = \{1, \ldots, n\}\) and \(S_n\) denotes the symmetric group on \(n\) elements - i.e. the permutations on \(n\) elements. If \(v = (v_1, \ldots, v_n) \in \mathbb{R}^n\) and \(\sigma \in S_n\), then \(\sigma(v) = (v_{\sigma(1)}, \ldots, v_{\sigma(n)}).\) \vskip5pt

\noindent
We write \(\mathcal{X}\) and \(\mathcal{Y}\) to denote the (measurable) spaces of predictors and responses, respectively. We write \(\mathcal{Z} = \mathcal{X} \times \mathcal{Y}.\) Typically, we think of \(\mathcal{X} = \mathbb{R}^p\) for some positive integer \(p\) and \(\mathcal{Y} = \mathbb{R}\) in the regression setting and \(\mathcal{Y} = [K]\), for some positive integer \(K\), in the classification setting.



\label{sec:intro}

\section{Foundations of Conformal Prediction}
\label{sec:foundations_of_CP}

\noindent
In this chapter, we introduce the key mathematical tools for this essay. The main theoretical result of this section is \cref{thm:fullconformal_coverage} which establishes the coverage guarantee for full conformal prediction.

\subsection{Quantiles and exchangeability}
\label{subsec:quantiles_and_exchangeability}

In the following definitions, we introduce the notions of quantiles, empirical cumulative distribution functions and exchangeability, which are fundamental to conformal prediction. The results in this subsection are stated as facts in \cite{angelopoulos2024theoreticalfoundationsconformalprediction}, and we provide our own proofs for these. The proof of \cref{lemma:exch_cdfquantile} follows the proof of Lemma 1 in \cite{romano2019_CQR}.

\begin{definition}[Exchangeability]
    The random variables \(Z_1, ..., Z_n\) are exchangeable if for all \(\sigma \in S_n\), we have that \[(Z_1, \ldots, Z_n) \overset{d}{=} (Z_{\sigma(1)}, \ldots, Z_{\sigma(n)}).\] Equivalently, \(Z_1, ..., Z_n\) are exchangeable if for any measurable set \(A\) and for any \(\sigma \in S_n\), we have that \[\Prob{(X_1, \ldots, X_n) \in A} = \Prob{(X_{\sigma(1)}, \ldots, X_{\sigma(n)}}) \in A.\]
\label{defn:exch}
\end{definition}

\begin{remark}
Note that if \(Z_1, \ldots, Z_n\) are exchangeable random variables taking values in \(\mathcal{Z}\), then they must be identically distributed. Indeed, for any measurable set \(A \in \mathcal{Z}\) and \(i \in [n]\), we have that \begin{align*} 
    \Prob{Z_i \in A} &= \Prob{(Z_1, \ldots, Z_{i-1}, Z_i, Z_{i+1} \ldots, Z_n) \in \mathcal{Z} \times \cdots \times \mathcal{Z} \times A \times \mathcal{Z} \times \ldots \times \mathcal{Z}} \\
    &= \Prob{(Z_i, \ldots, Z_{i-1}, Z_1, Z_{i+1} \ldots, Z_n) \in \mathcal{Z} \times \cdots \times \mathcal{Z} \times A \times \mathcal{Z} \times \ldots \times \mathcal{Z}} \\
    &= \Prob{Z_1 \in A},
\end{align*} where we use exchangeability to obtain the second equality. However, exchangeable random variables need not be independent. Indeed, if \(Z_1, \ldots, Z_n\) are sampled without replacement from the set \([n]\), then they are exchangeable, since any particular realisation has probability \(\frac{1}{n!}\), but \(Z_1, \ldots, Z_n\) are certainly not independent. Therefore, we see that exchangeability is a weaker condition that being i.i.d.
\label{rmk:exch_dist}
\end{remark}

\begin{remark}
    Another way to view exchangeability is as follows. Suppose the random variables \(Z_1, \ldots, Z_n \in \mathbb{R}\) are almost surely distinct and exchangeable. Taking \(A = \{(z_1, \ldots, z_n) \in \mathbb{R}^n : z_1 < z_2 < \cdots < z_n \}\), we have that for any \(\sigma \in S_n\), \[\Prob{Z_1 < \cdots < Z_n} = \Prob{(Z_1, \ldots, Z_n) \in A} = \Prob{(Z_{\sigma(1)}, \ldots, Z_{\sigma(n)}) \in A} = \Prob{Z_{\sigma(1)} < \cdots < Z_{\sigma(n)}}.\] This means that \(Z_1, \ldots, Z_n\) are equally likely to appear in any given ordering.
\label{rmk:exch_ordering}
\end{remark}

% \begin{definition}
%     Let \(P\) be a probability distribution on \(\mathbb{R}\) with cumulative distribution function \(F\). The \textit{quantile function} of \(P\) is defined for \(\beta \in (0,1)\) by \[Q(P;\beta) \coloneqq \inf\left\{ z \in \mathbb{R}: F(z) \geq \beta \right\}.\] We may also use the notation \(Q(F;\beta)\) in place of \(Q(P;\beta)\).
% \label{defn:prob_quantile}
% \end{definition}

\begin{definition}
    Let \(w = (w_1, \ldots, w_n) \in [0,1]^n\) be such that \(\sum_{i=1}^n w_i = 1\) and let \(z \in \mathbb{R}^n\). We define the following quantities. \begin{enumerate}[(i)] \itemsep0em
        \item The \textit{weighted empirical cumulative distribution function} of \(z\) with respect to the \textit{weights} \(w\) is the function \(F^w_z: (-\infty, \infty] \to [0,1]\) given for \(x \in (-\infty, \infty]\) by \[\hat{F}^{w}_z(x) \coloneqq \sum_{i=1}^n w_i \Ind{z_i \leq x}.  \] We define \(\hat{F}^w_z(\infty) = 1\).
        \item The \textit{empirical cumulative distribution function} of \(z\) is the function \(\hat{F}_z: (-\infty, \infty] \to [0,1]\) given for \(x \in (-\infty, \infty]\) by \[\hat{F}_z(x) = \hat{F}^{(1/n, \ldots, 1/n)}(x) = \frac{1}{n}\sum_{i=1}^n \Ind{z_i \leq x}.\]
        \item The \textit{weighted quantile function} \(\hat{Q}_z:(0,1] \to (-\infty, \infty]\) of \(z\) with respect to the weights \(w\) is defined for \(\beta \in (0,1]\) by \[\hat{Q}_z(\beta) \coloneqq \inf \left\{ x \in (-\infty, \infty]: \hat{F}^w_z(x) \geq \beta \right\}.\]
        \item The \textit{quantile function} \(\hat{Q}_z:(0,1] \to (-\infty, \infty]\) of \(z\) is defined for \(\beta \in (0,1]\) by \[\hat{Q}_z(\beta) \coloneqq \hat{Q}^{(1/n, \ldots, 1/n)}_z(\beta).\]
    \end{enumerate}
\label{defn:empirical_cdfquantile}
\end{definition}

\noindent
The weighted versions of the quantities above will play an important role in \cref{sec:extensions_of_CP}. Therefore, some of the results of this section are proven for the weighted quantities even if only the special case \(w = (1/n, \ldots, 1/n)\) is required. The following lemma formalises a sense in which the empirical cumulative distribution and quantile functions are inverses of each other.

\begin{lemma} Let \(z \in \mathbb{R}^n\), \(w = (w_1, \ldots, w_n) \in [0,1]^n\) be such that \(\sum_{i=1}^n w_i = 1\) and \(\beta \in (0,1)\).
    \begin{enumerate}[(i)] \itemsep0em
        \item We have that \(\hat{F}^w_z\left(\hat{Q}^w_z(\beta \right) \geq \beta\).
        \item If the components of \(z\) are distinct, then \(\hat{F}_z\left(\hat{Q}_z(\beta)\right) = \frac{\lceil{n\beta}\rceil}{n}.\)
    \end{enumerate} 
\label{lemma:cdfquantile}
\end{lemma}
\begin{proof}

    \begin{enumerate}[(i)] \itemsep0em
        \item The first claim follows from upon noting that \(\hat{F}^w_z\) is right-continuous. Indeed, by the definition of the infimum, there exists a sequence \((x_m)\) of real numbers satisfying \(\hat{F}^w_z(x_m) \geq \beta\) for all \(m\) and \(x_m \downarrow \hat{Q}^w_z(\beta)\) as \(m \to \infty\). By the right continuity of \(\hat{F}^w_z\), it follows that \(\hat{F}^w_z\left(\hat{Q}^w_z(\beta) \right) \geq \beta\).
        \item We observe that for all \(x \in \mathbb{R}\), we have that \(\hat{F}_z(x) \in \left\{0, \frac{1}{n}, \ldots, \frac{n-1}{n}, 1\right\}\). Since the components of \(z = (z_1, \ldots, z_n)\) are distinct, \(\hat{F}_z\) jumps by \(\frac{1}{n}\) at each \(z_i\), for \(i \in [n]\). Therefore \[\hat{F}_z\left(\hat{Q}_z(\beta)\right) = \frac{1}{n} \inf \{k \in \{0\} \cup [n]: \ k/n \geq \beta \} = \frac{\lceil{\beta n}\rceil}{n}.\]
    \end{enumerate}
\end{proof}

\noindent
We now prove a lemma which is fundamental in the proof of the coverage guarantee in section \cref{subsec:fullconformal} and links all three of the concepts introduced above.

\begin{lemma}
    If the random variables \(Z_1, \ldots, Z_n\) are exchangeable, then for any \(i \in [n]\) and \(\beta \in (0,1)\), we have \[\Prob{Z_i \leq \hat{Q}_Z (\beta)} \geq \beta,\] where \(Z \coloneqq (Z_1, \ldots, Z_n)\). If, moreover, \(Z_1, \ldots, Z_n\) are almost surely distinct, then \[\Prob{Z_i \leq \hat{Q}_Z(\beta)} = \frac{\lceil{\beta n}\rceil}{n}.\]
\label{lemma:exch_cdfquantile}
\end{lemma}
\begin{proof}
    Fix \(\beta \in (0,1)\). We first claim that the exchangeability of \(Z_1, \ldots, Z_n\) implies that \[\Prob{Z_j \leq \hat{Q}_Z(\beta)} = \Prob{Z_1 \leq \hat{Q}_Z(\beta)}, \] for any \(j \in [n]\). Fix \(j \in [n]\) and define \(S_\beta = \left\{ y \in \mathbb{R}^n : \ y_j \leq \hat{Q}_y(\beta) \right\}.\) Define \(\tau \in S_n\) to be the transposition exchanging \(1\) and \(j\). We have that  
    \begin{align*}
        \Prob{Z_j \leq \hat{Q}_Z(\beta)} &= \Prob{(Z_1, \ldots, Z_n) \in S_\beta} \\
        &= \Prob{(Z_{\tau(1)}, \ldots, Z_{\tau(n)}) \in S_\beta} \\
        &= \Prob{Z_1 \leq \hat{Q}_Z(\beta)},
    \end{align*} where the second equality follows from exchangeability. This proves our claim. \vskip 5pt
    
    \noindent
    To complete the proof, we use the deterministic result from \cref{lemma:cdfquantile}. By the claim shown above, we have that \begin{align*}
        \Prob{Z_i \leq \hat{Q}_Z(\beta)} = \frac{1}{n} \sum_{j=1}^n \Prob{Z_j \leq \hat{Q}_Z(\beta)} = \mathbb{E} \left[ \frac{1}{n} \sum_{j=1}^n \Ind{Z_j \leq \hat{Q}_Z(\beta)} \right]
        = \Exp{\hat{F}_Z(\hat{Q}_Z(\beta))}{[}{]}.
    \end{align*} By \cref{lemma:cdfquantile}, we have that \(\hat{F}_Z(\hat{Q}_Z(\beta)) \geq \beta\). Moreover, if \(Z_1, \ldots, Z_n\) are almost surely distinct, then \(\hat{F}_Z(\hat{Q}_Z(\beta)) = \frac{\lceil{\beta n}\rceil}{n}\) almost surely. This proves the lemma.
\end{proof}

\begin{remark}
    In the case where \(Z_1, \ldots, Z_n\) are almost surely distinct, we can obtain the above result using an even simpler argument. Fix \(i \in [n]\). As discussed in \cref{rmk:exch_ordering}, each of the \(n!\) orderings of \(Z = (Z_1, \ldots, Z_n)\) are equally likely. For any \(k \in [n]\), Since there are \((n-1)!\) orderings where \(Z_i\) is the \(k^\mathrm{th}\) smallest element of \(Z\), we deduce that the probability that \(Z_i\) is the \(k^\mathrm{th}\) smallest element of \(Z\) is \(1/n\). Summing over \(k\) ranging from \(1\) to \(\lceil \beta n \rceil\) gives that \[\Prob{Z_i \leq \hat{Q}_Z(\beta)} = \frac{\lceil{\beta n}\rceil}{n}.\]
\label{rmk:exch_rank_argument}
\end{remark}

\subsection{Full conformal prediction}
\label{subsec:fullconformal}

In this subsection, we present the full conformal prediction algorithm and prove its coverage guarantee. The presentation of the material in this subsection and the proof of \cref{thm:fullconformal_coverage} is inspired by \cite{angelopoulos2024theoreticalfoundationsconformalprediction}. \vskip5pt

\noindent
We first introduce the notion of a \textit{conformity score}. A conformity score is a function \[s:\mathcal{Z} \times \cup_{j \geq 1} \mathcal{Z}^j \to \mathbb{R}.\]

\begin{remark}
    Whilst the conformity score may theoretically be an arbitary function - as in the above display - we now explain how the conformity score should be understood in practice. The first argument of the conformity score represents an arbitary test point and the second argument a training dataset. The conformity score measures the discrepancy between the test point and a model fitted using the training dataset, where a high conformity score indicates that the test point "conforms" poorly with the fitted model. In particular, we note that computing the conformity score involves fitting a model using the data in the second argument. Following this intuition, we will also refer to a value of \(s\) given by \(s(z;D)\) as the conformity score of the test point \(z\) with respect to the data \(D\).
\label{rmk:conformity_score_intuition}
\end{remark}

\begin{example}
\label{example:absolute_residual_score}
    Consider the regression setting where \(\mathcal{X} = \mathbb{R}^p\) and \(\mathcal{Y} = \mathbb{R}\) for some positive integer \(p\). Suppose \((X_1, Y_1), \ldots, (X_n, Y_n) \in \mathcal{Z}\) and each \((X_i, Y_i) \sim P\) for \(i \in [n]\). Suppose \((X,Y) \sim P\) is independent of \(((X_i, Y_i))_{i=1}^n\) and \(\hat{\mu}: \mathcal{X} \to \mathcal{Y}\) is an estimate of the regression function \(x \mapsto \Exp{Y|X = x}{(}{)}\) based on \(((X_i, Y_i))_{i=1}^n\). \vskip5pt

    \noindent
    An important example of a conformity score in this case is the \textit{absolute residual score} given by \[s\left(\left(x,y); ((X_1, Y_1), \ldots, (X_n, Y_n)\right)\right) = |y - \hat{\mu}(x)|.\]
\end{example} \textcolor{red}{Example in classification setting?}
\textcolor{red}{Words before definition below.} 

\noindent
We now introduce the notion of a symmetric conformity score, which is one that does not depend on the order in which the training data points are provided.

\begin{definition}
    A conformity score \(s\) is \textit{symmetric} if for any \(z \in \mathcal{Z}\), \(D \in \cup_{j \geq 1} \mathcal{Z}^j\) and \(j \in \mathbb{N}\), we have that \[s(z; D) = s(z; \sigma(D)).\]
\label{defn:symmetric_conformityscore}
\end{definition}

\noindent
We now give a brief informal description of how the coverage guarantee is obtained. We will consider the conformity score of each \((X_i, Y_i)\) with respect to \(((X_i, Y_i))_{i=1}^{n+1}\). We will show that if \(s\) is symmetric, then these conformity scores are exchangeable. As discussed in \cref{rmk:exch_ordering} and \cref{rmk:exch_rank_argument}, this means that any ordering of the conformity scores is equally likely, so the probability that the conformity score of the test point \((X_{n+1}, Y_{n+1})\) lies in the bottom \(1-\alpha\) fraction of all the conformity scores is at least \(1-\alpha\). This is the primary idea used to construct the prediction set. However, since \(Y_{n+1}\) is unknown, will instead consider the test point \((X_{n+1}, y)\) and select those \(y \in \mathcal{Y}\) to be included the prediction set, whose conformity score falls in the bottom \(1-\alpha\) fraction. \vskip5pt

\noindent
Before formally proving the coverage guarantee, we introduce the notation for the quantities mentioned in the outline above. Write \[D = ((X_i, Y_i))_{i=1}^{n+1}, \quad \text{and} \quad D^{y} = ((X_1, Y_1), \ldots, (X_n, Y_n), (X_{n+1}, y)), \] for any \(y \in \mathcal{Y}\).  Write \[S_i = s((X_i, Y_i); D), \ \ S_i^y = s((X_i, Y_i); D^y), \] and \[S_{n+1}^y = s((X_{n+1}, y); D^y).\] Additionally, let \[S^y = (S_1^y, \ldots, S_n^y, S_{n+1}^y).\]

\begin{theorem}
    Suppose \((X_1, Y_1), \ldots, (X_{n+1}, Y_{n+1}) \in \mathcal{Z}\) are exchangeable and \(s\) is a symmetric conformity score. Define the prediction set \begin{equation}
        C(X_{n+1}) = \left\{ y \in \mathcal{Y}: \ \ S_{n+1}^y \leq \hat{Q}_{S^y}(1-\alpha) \right\}.
    \label{eqn:fullconformal_prediction_set}
    \end{equation} Then we have that \[\Prob{Y_{n+1} \in C(X_{n+1})} \geq 1-\alpha.\] If, moreover, the scores \(S_1, \ldots, S_{n+1}\) are almost surely distinct, then we have that \[\Prob{Y_{n+1} \in C(X_{n+1})} \leq 1-\alpha + \frac{1}{n+1}.\]

\label{thm:fullconformal_coverage}  
\end{theorem}

\begin{proof}
    We begin by showing that \(S \coloneqq (S_1, \ldots, S_{n+1})\) is exchangeable, as mentioned in the discussion above. \vskip5pt
    
    \noindent
    Making the dependence on \(D\) explicit, note that \(S_i = s((X_i, Y_i); D)\) and \(S_{\sigma(i)} = s((X_{\sigma(i)}, Y_{\sigma(i)}); D)\) for any \(i \in [n+1]\) and \(\sigma \in S_{n+1} \). Define also the function \(h: \mathcal{Z}^{n+1} \to \mathbb{R}^{n+1}\) by \[h:z \mapsto (s(z_1;z), \ldots, s(z_{n+1};z)),\] for any \(z = (z_1, \ldots, z_n) \in \mathcal{Z}^n\). Then we have that for any measurable set \(A\), 
    \begin{align*}
        \Prob{\left(S_{\sigma(1)}, \ldots, S_{\sigma(n+1)}\right) \in A} &= \Prob{\left( s((X_{\sigma(1)}, Y_{\sigma(1)}); D), \ldots, s((X_{\sigma(n+1)}, Y_{\sigma(n+1)}); D)  \right) \in A} \\
        &= \Prob{\left( s((X_{\sigma(1)}, Y_{\sigma(1)}); \sigma(D)), \ldots, s((X_{\sigma(n+1)}, Y_{\sigma(n+1)}); \sigma(D))  \right) \in A} \\
        &= \Prob{ h(Z_{\sigma(1)}, \ldots, Z_{\sigma(n+1)}) \in A},
    \end{align*} where the second equality follows from the symmetry of the score function. By the exchangeability of \((Z_1, \ldots, Z_n)\), we have that \begin{align*}
        \Prob{ h(Z_{\sigma(1)}, \ldots, Z_{\sigma(n+1)}) \in A} &= \Prob{ (Z_{\sigma(1)}, \ldots, Z_{\sigma(n+1)}) \in h^{-1}(A)} \\
        &= \Prob{ (Z_{1}, \ldots, Z_{n+1}) \in h^{-1}(A)} \\
        &= \Prob{ h(Z_{1}, \ldots, Z_{n+1}) \in A} \\
        &= \Prob{(S_1, \ldots, S_{n+1}) \in A}, 
    \end{align*} which shows that \[\Prob{\left(S_{\sigma(1)}, \ldots, S_{\sigma(n+1)}\right) \in A} = \Prob{(S_1, \ldots, S_{n+1}) \in A}, \] so \(S\) is exchangeable. \vskip5pt

    \noindent
    Finally, note that \[Y_{n+1} \in C(X_{n+1}) \iff S_{n+1} \leq \hat{Q}_S(1-\alpha)\] by the definition of \(C(X_{n+1})\). Therefore, \cref{lemma:exch_cdfquantile} implies that \[\Prob{Y_{n+1} \in C(X_{n+1})} \geq 1-\alpha, \] and that if \(S_1, \ldots, S_{n+1}\) are almost surely distinct, then \[\Prob{Y_{n+1} \in C(X_{n+1})} = \frac{\lceil (1-\alpha) (n+1) \rceil }{n+1} \leq 1-\alpha + \frac{1}{n+1}.\]
\end{proof}

\noindent
We now derive an equivalent characterisation of the prediction set that we will use in the remainder of this essay and that will be particularly useful in the next subsection. In order to do this, we first present a lemma.

% \begin{lemma}
%     For any \(z \in \mathbb{R}^n\), \(y \in \mathbb{R}\) and \(\beta \in (0,1]\), we have that \[y \leq \hat{Q}_{(z,y)} (\beta) \iff y \leq \begin{cases}
%         \hat{Q}_z\left( \frac{\lceil \beta (n+1) \rceil }{n} \right) \quad &\R{if} \ \frac{\lceil \beta(n+1) \rceil}{n} \leq 1, \\
%         \infty \quad &\R{otherwise}.
%     \end{cases} \]
% \label{lemma:quantile_lemma}
% \end{lemma}

% \begin{proof}
%     First suppose that \(y \leq \hat{Q}_{(z,y)}(\beta)\). Note that by \cref{defn:empirical_cdfquantile}, we have that \[\hat{F}_{(z,y)} (x) = \frac{1}{n+1} \left( \sum_{i=1}^n \Ind{z_i \leq x} + \Ind{y \leq x} \right) = \frac{n}{n+1} \hat{F}_z(x) + \frac{1}{n+1}\Ind{y \leq x}.\] If \( \frac{\lceil \beta (n+1) \rceil}{n} \leq 1\), then it follows that \[\hat{F}_{(z,y)}\left(  \hat{Q}_z \left( \frac{\lceil \beta (n+1) \rceil }{n} \right) \right) \geq \frac{n}{n+1} \hat{F}_z \left( \hat{Q}_z \left( \frac{\lceil \beta (n+1) \rceil }{n} \right) \right) \geq \frac{n}{n+1} \frac{\beta(n+1)}{n} \geq \beta \] by \cref{lemma:cdfquantile}. We deduce that \begin{align}
%         \hat{Q}_{(z,y)}(\beta) \leq \hat{Q}_z \left( \frac{\lceil \beta (n+1) \rceil }{n} \right).
%     \label{eqn:quantile_lemma_disp1}
%     \end{align} Clearly \(y \leq \infty\), so this proves the forward direction of the lemma. To prove the converse direction, first note that if \(y > \hat{Q}_{(z,y)}(\beta)\), then we have that \begin{align}
%         \hat{F}_z(\hat{Q}_{(z,y)}(\beta)) &= \frac{n+1}{n} \hat{F}_{(z,y)}(\hat{Q}_{(z,y)}(\beta)) - \frac{1}{n} \Ind{y \leq \hat{Q}_{(z,y)}(\beta)} \label{eqn:quantile_lemma_disp2}\\
%         &= \frac{n+1}{n} \hat{F}_{(z,y)} (\hat{Q}_{(z,y)} (\beta) ) \nonumber\\
%         &= \frac{n+1}{n} \hat{F}_{(z,y)} \left(  \hat{Q}_{(z,y)} \left( \frac{\lceil \beta (n+1) \rceil }{n+1} \right) \right) \nonumber\\
%         &\geq \frac{\lceil \beta (n+1) \rceil }{n} \nonumber, 
%     \end{align} where the final inequality follows from \cref{lemma:cdfquantile}. If \(\frac{\lceil \beta (n+1) \rceil}{n} \leq 1\), then this implies that \[y > \hat{Q}_{(z,y)}(\beta) \geq \hat{Q}_z \left( \frac{\lceil \beta (n+1) \rceil}{n} \right), \] a contradiction, so \(y \leq \hat{Q}_{(z,y)}(\beta)\). If \(\frac{\lceil\beta  (n+1) \rceil}{n} > 1\), then it implies \(\hat{F}_z(\hat{Q}_{(z,y)}(\beta)) > 1\), a contradiction, so \(y \leq \hat{Q}_{(z,y)}(\beta).\)
% \end{proof}

% \begin{remark}
%     Note that for any \(z \in \mathbb{R}^n\) and \(\beta \in (0,1]\), \[\hat{Q}_{(z,\infty)} (\beta) = \begin{cases}
%         \hat{Q}_z\left( \frac{\lceil \beta (n+1) \rceil }{n} \right) \quad &\R{if} \ \frac{\lceil \beta(n+1) \rceil}{n} \leq 1, \\
%         \infty \quad &\R{otherwise}.
%         \end{cases}\] This follows from \eqref{eqn:quantile_lemma_disp1} and \eqref{eqn:quantile_lemma_disp2} upon noting that \(\hat{Q}_{(z,\infty)}(\beta) < \infty\) if \(\beta \leq \frac{n}{n+1}\), which is equivalent to \(\frac{\lceil \beta (n+1) \rceil}{n} \leq 1\).
% \end{remark}

\begin{lemma}
    Let \(z \in \mathbb{R}^n\), \(y \in \mathbb{R}\) and \(w \in [0,1]^n\) such that \(\sum_{i=1}^n w_i = 1\). We have that for any \(\beta \in (0,1)\), \[y \leq \hat{Q}^w_{(z,y)}(\beta) \iff y \leq \hat{Q}^w_{(z,\infty)}(\beta).\]
\label{lemma:quantile_lemma}
\end{lemma}
\begin{proof}
    First note that \[
        \hat{F}^w_{(z,\infty)}(x) = \sum_{i=1}^n w_i \Ind{x \geq z_i} + w_{n+1} \Ind{x \geq \infty} \leq \sum_{i=1}^n w_i \Ind{x \geq z_i} + w_{n+1} \Ind{x \geq y} = \hat{F}^w_{(z,y)}(x).
    \] Therefore, \(\hat{F}^w_{(z,y)}(\hat{Q}^w_{(z, \infty)}(\beta)) \geq \hat{F}^w_{(z,\infty)}(\hat{Q}^w_{(z, \infty)}(\beta)) \geq \beta\), which implies that \begin{equation}
        \hat{Q}^w_{(z, \infty)}(\beta) \geq \hat{Q}^w_{(z, y)}(\beta).
    \label{eqn:quantile_lemma_pt1}
    \end{equation} This proves the forward direction of the lemma. Now suppose that \(y > \hat{Q}^w_{(z,y)}(\beta).\) Then we have that \begin{align*}
        \hat{F}^w_{(z,\infty)}(\hat{Q}^w_{(z, y)}(\beta)) &= \sum_{i=1}^n w_i \Ind{\hat{Q}^w_{(z, y)}(\beta) \geq z_i} + w_{n+1} \Ind{\hat{Q}^w_{(z, y)}(\beta) \geq \infty} \\
        &= \hat{F}^w_{(z,y)}(\hat{Q}^w_{(z, y)}(\beta)) + w_{n+1} \left( \Ind{\hat{Q}^w_{(z, y)}(\beta) \geq \infty} - \Ind{\hat{Q}^w_{(z, y)}(\beta) \geq y} \right) \\
        &= \hat{F}^w_{(z,y)}(\hat{Q}^w_{(z, y)}(\beta)) \geq \beta.
    \end{align*} This implies that \[\hat{Q}^w_{(z, y)}(\beta) \geq \hat{Q}^w_{(z, \infty)}(\beta),\] which together with \eqref{eqn:quantile_lemma_pt1}, also implies \[\hat{Q}^w_{(z, y)}(\beta) = \hat{Q}^w_{(z, \infty)}(\beta).\] Thus, \(y > \hat{Q}^w_{(z, \infty)}(\beta)\), which completes the proof of the lemma.
\end{proof}

\begin{lemma}
    Let \(z \in \mathbb{R}^n\) and \(\beta \in (0,1)\). Then we have that \[\hat{Q}_{(z,\infty)} (\beta) = \begin{cases}
        \hat{Q}_z\left( \frac{\lceil \beta (n+1) \rceil }{n} \right) \quad &\R{if} \ \frac{\lceil \beta(n+1) \rceil}{n} \leq 1, \\
        \infty \quad &\R{otherwise}.
        \end{cases}\]
\label{lemma:quantile_lemma_extra}
\end{lemma}
\begin{proof}
    First note that the condition \(\frac{\lceil \beta(n+1) \rceil}{n} \leq 1\) is equivalent to \(\beta \leq \frac{n}{n+1}\). If \(\beta > \frac{n}{n+1}\), then \(\hat{Q}_{(z, \infty)}(\beta) = \infty\) since \(\frac{1}{n+1}\sum_{i=1}^n \Ind{x \geq z_i} \leq \frac{n}{n+1}.\) Now observe that for any \(x < \infty\), we have that \[\hat{F}_{(z, \infty)}(x) = \frac{1}{n+1} \Ind{x \geq z_i} + \frac{1}{n+1}\Ind{x \geq \infty} = \frac{n}{n+1} \hat{F}_z(x).\] If \(\beta \leq \frac{n}{n+1}\), we have that \[\frac{\beta(n+1)}{n} \leq \frac{n+1}{n}  \hat{F}_{(z,\infty)}(\hat{Q}_{(z,\infty)}(\beta)) = \hat{F}_{z}(\hat{Q}_{(z,\infty)}(\beta)),\] which implies that \[\hat{Q}_{(z,\infty)}(\beta) \geq \hat{Q}_{z}\left(\frac{\beta(n+1)}{n}\right) = \hat{Q}_z\left( \frac{\lceil \beta (n+1) \rceil }{n} \right).\] We also have that \[\hat{F}_{(z, \infty)}\left( \hat{Q}_z\left( \frac{\lceil \beta (n+1) \rceil }{n} \right) \right) = \frac{n}{n+1} \hat{F}_{z}\left( \hat{Q}_z\left( \frac{\lceil \beta (n+1) \rceil }{n} \right) \right) = \frac{n}{n+1}\frac{\lceil \beta (n+1) \rceil }{n} \geq \beta. \] This shows that \[\hat{Q}_z\left( \frac{\lceil \beta (n+1) \rceil }{n} \right) = \hat{Q}_{(z, \infty)}(\beta).\]
\end{proof}

\begin{remark}
    The key implication of \cref{lemma:quantile_lemma_extra} is that we can reformulate the inequality in \eqref{eqn:fullconformal_prediction_set} in a way that only the left-hand-side depends on \(S_{n+1}^y\), which is stated in \cref{thm:fullconformal_coverage_v2} below. This will be particularly important in \cref{subsec:splitconformal}. 
\end{remark}

\noindent The following reformulation of \cref{thm:fullconformal_coverage} is an immediate consequence of \cref{lemma:quantile_lemma}.

\begin{theorem}
    Suppose \((X_1, Y_1), \ldots, (X_{n+1}, Y_{n+1}) \in \mathcal{Z}\) are exchangeable and \(s\) is a symmetric conformity score. Define the prediction set \begin{equation}
        C(X_{n+1}) = \left\{ y \in \mathcal{Y}: \ \ S_{n+1}^y \leq \hat{Q}_{(S_1^y, \ldots, S_n^y, \infty)}(1-\alpha)  \right\}.
    \label{eqn:fullconformal_prediction_set_v2}
    \end{equation} Then we have that \[\Prob{Y_{n+1} \in C(X_{n+1})} \geq 1-\alpha.\] If, moreover, the scores \(S_1, \ldots, S_{n+1}\) are almost surely distinct, then we have that \[\Prob{Y_{n+1} \in C(X_{n+1})} \leq 1-\alpha + \frac{1}{n+1}.\]

\label{thm:fullconformal_coverage_v2}  
\end{theorem}


\begin{remark}
    Firstly, we note that in the case \(\frac{\lceil (1-\alpha)(n+1) \rceil}{n} > 1\), we have that \(C(X_{n+1}) = \mathcal{Y}\) since \(\hat{Q}_{(\hat{S}_1, \ldots, \hat{S}_n, \infty)} = \infty\), which trivially covers all test points. However, since \(\frac{\lceil (1-\alpha)(n+1) \rceil}{n} > 1\) is equivalent to \(\alpha < \frac{1}{n+1}\), we can ignore this situation provided \(\alpha\) and \(n\) are not too small. Secondly, we note that if \(z \in \mathbb{R}^n\), then \begin{align*}
        \hat{Q}_z\left( \frac{\lceil (1-\alpha)(n+1) \rceil}{n} \right) = \inf \left\{ y \in \mathbb{R}: \, \sum_{i=1}^{n} \Ind{z_i \leq y} \geq \lceil (1-\alpha)(n+1) \rceil \right\} =  z_{(\lceil (1-\alpha)(n+1) \rceil)}, 
    \end{align*} where \(z_{(1)} \leq z_{(2)} \leq \cdots \leq z_{(n)}\) are the \textit{order statistics} of \(z\), defined by \[z_{(k)} = \inf \left\{ y \in \mathbb{R}: \, \sum_{i=1}^{n} \Ind{z_i \leq y} \geq k \right\},\] for \(k \in [n]\). This means the calculation of the quantile \(\hat{Q}_{(\hat{S}_1, \ldots, \hat{S}_n, \infty)}\) is simplified to finding the \(\lceil (1-\alpha)(n+1) \rceil\) smallest element in the list \((\hat{S_1}, \ldots, \hat{S}_n)\). This leads to \cref{alg:fullconformal} below.
\end{remark}


\begin{algorithm}[H]
\caption{Full conformal prediction algorithm}
\label{alg:fullconformal}
\begin{algorithmic}
    \Require Data \(((X_i, Y_i))_{i=1}^n\); test predictor \(X_{n+1}\); miscoverage level \(\alpha \in (0,1)\), conformity score \(s\).
    \State Initialise \(C \gets \emptyset\)
    \If{\(\lceil (1-\alpha)(n+1) \rceil > n\)}
        \State \(C = \mathcal{Y}\)
    \Else
        \For{\(y \in \mathcal{Y}\)}
            \State Compute \(S_i^y = s((X_i, Y_i); D^y)\).
            \State Compute \(S_{n+1}^y = s((X_{n+1}, y); D^y)\).
            \State Set \(S^y = (S_1^y, \ldots, S_{n}^y)\).
            \State Compute \(\hat{Q}\) as the \(\lceil (1-\alpha)(n+1) \rceil \) element in the list \(S_1^y, \ldots, S_{n}^y\).
            \If{\(S_{n+1}^y \leq \hat{Q}\)}
                \State \(C \gets C \cup \{y\} \).
            \EndIf      
        \EndFor
    \EndIf
    \Ensure \(C\)
\end{algorithmic}
\end{algorithm}

\begin{remark}
    \textcolor{red}{Something about the prediction set above not being obviously computable. Can do in specific cases (need references) but may not be possible in general. In above algo, if \(\mathcal{Y}\) is not discrete, problematic, need to discretise. Also, the form of the prediction set not at all obvious Split conformal resolves some of this... (e.g. below we will see we can get interval). Dependence of the form of the prediction set on the conformity score also not obvious.}
\end{remark}

\subsection{Split conformal prediction}
\label{subsec:splitconformal}

In this subsection, we will present the split conformal prediction algorithm. We will show that it is, in fact, a special case of full conformal prediction, and so the coverage guarantee from \cref{subsec:fullconformal} also holds for split conformal prediction. We will also compare full and split conformal prediction, discussing their respective advantages and disadvantages.\vskip5pt

\noindent
For split conformal prediction, we assume that we are given a function \(\hat{s}:\mathcal{X} \times \mathcal{Y} \to \mathbb{R}\) that depends on a \textit{proper training set} \(D_\mathrm{tr} \in \cup_{j \geq 1} \mathcal{Z}^j \) that is disjoint from the calibration data \(D_\mathrm{cal} \coloneqq ((X_i, Y_i))_{i=1}^n\).  Split conformal prediction then uses the calibration data together with \(\hat{s}\) and \(X_{n+1}\), to form a prediction set for \(Y_{n+1}\). \vskip5pt

\noindent
Before proving the coverage guarantee for split conformal prediction, we introduce the necessary notation. Let \(\hat{S}_i = \hat{s}(X_i, Y_i)\) for \(i \in [n]\) and let \(\hat{S}_{n+1}^y = \hat{s}(X_{n+1}, y)\) for any \(y \in \mathcal{Y}\).

\begin{theorem}
    Suppose \((X_1, Y_1), \ldots, (X_n, Y_n), (X_{n+1}, Y_{n+1}) \in \mathcal{Z}\) are exchangeable. Define the prediction set \begin{equation}
        C(X_{n+1}) = \left\{ y \in \mathcal{Y}: \  \hat{S}_{n+1}^y \leq \hat{Q}_{(\hat{S}_1, \ldots, \hat{S}_n, \infty)}(1-\alpha)  \right\}.
    \label{eqn:splitconformal_prediction_set}
    \end{equation} Then we have that \[\Prob{Y_{n+1} \in C(X_{n+1})} \geq 1-\alpha.\] If, moreover, the scores \(\hat{S}_1, \ldots, \hat{S}_{n+1}\) are almost surely distinct, then we have that \[\Prob{Y_{n+1} \in C(X_{n+1})} \leq 1-\alpha + \frac{1}{n+1}.\]
\label{thm:splitconformal_coverage}
\end{theorem}
\begin{proof}
    We work conditional on \(D_\mathrm{tr}\), so we may treat \(\hat{s}\) as a non-random function. The key observation is that since \cref{thm:fullconformal_coverage_v2} holds for any symmetric conformity score, we may choose the conformity score to be independent of its second argument. Define the conformity score \(s(z; \tilde{D}) = \hat{s}(z)\) for all \(z \in \mathcal{Z}\) and \(\tilde{D} \in \cup_{j \geq 1} \mathcal{Z}^j\). Since this is independent of \(\tilde{D}\), it is certainly symmetric. In the notation of \cref{thm:fullconformal_coverage_v2}, we then have that \[S_i = S_i^y = \hat{S}_i \ \ \mathrm{and} \ \ S_{n+1}^y = \hat{s}(X_{n+1}, y),\] for all \(y \in \mathcal{Y}\). Therefore, the prediction set in \cref{thm:fullconformal_coverage_v2} takes exactly the form stated above, so we have that \[\Prob{Y_{n+1} \in C(X_{n+1}) | D_{\mathrm{tr}}  } \geq 1-\alpha.\] If \(\hat{S}_1, \ldots, \hat{S}_n\) are almost surely distinct, then \[\Prob{Y_{n+1} \in C(X_{n+1}) | D_{\mathrm{tr}}  } \leq 1-\alpha + \frac{1}{n+1}.\] The result follows by marginalising over \(D_\mathrm{tr}.\)
\end{proof}

\begin{algorithm}[H]
\label{alg:split_conformal}
\caption{Split conformal prediction algorithm}
\begin{algorithmic}
    \Require Calibration \(((X_i, Y_i))_{i=1}^n\); test predictor \(X_{n+1}\); miscoverage level \(\alpha \in (0,1)\), conformity score \(\hat{s}\).
    \State Initialise \(C \gets \emptyset\)
    \If{\(\lceil (1-\alpha)(n+1) \rceil > n\)}
        \State \(C = \mathcal{Y}\)
    \Else
        \State Compute \(\hat{S}_i = \hat{s}(X_i,Y_i)\) for each \(i \in [n]\).
        \For{\(y \in \mathcal{Y}\)}
            \State Compute \(\hat{s}(X_{n+1}, y)\).
            \State Compute \(\hat{Q}\) as the \(\lceil (1-\alpha)(n+1) \rceil \) smallest element in the list \(\hat{S}_1, \ldots, \hat{S}_n\).
            \If{\(\hat{s}(X_{n+1}, y) \leq \hat{Q}\)}
                \State \(C \gets C \cup \{y\} \).
            \EndIf   
        \EndFor
    \EndIf
    \Ensure \(C\)
\end{algorithmic}
\end{algorithm}

\begin{remark}
    Although \(D_\mathrm{tr}\) and \(\hat{s}\) can be arbitary for the coverage guarantee to hold, in practice, we usually split the full training set into a proper training set \(D_\mathrm{tr}\) and a calibration set \(D_\mathrm{cal}\). We fit a model on the proper training set to obtain a conformity score \(\hat{s}\), and then apply the split conformal prediction procedure given in \cref{alg:split_conformal} to the calibration data to obtain the prediction set.
\end{remark}

\begin{remark}
    Note that the split conformal coverage guarantee imposes no restrictions on \(D_\mathrm{tr}\) and the fitting procedure used to obtain \(\hat{s}\), whereas full conformal prediction requires the conformity score to be symmetric. Moreover, split conformal prediction has the advantage that it is more computationally efficient than full conformal prediction. Indeed - as discussed in the above remark - in split conformal prediction, we must only fit the model once to obtain \(\hat{s}\). However, in full conformal prediction, we must refit the model for each \(y \in \mathcal{Y}\) since we compute the conformity score with respect to a dataset that includes the point \((X_{n+1}, y)\). On the other hand, full conformal prediction has the advantage that it uses all of the training data to fit the model, whereas in split conformal prediction, we may typically only use half of the full training set as the proper training set and the other half as the calibration set.
\end{remark}

\begin{example}
Consider the regression setting as in \cref{example:absolute_residual_score} and assume \(p = 1\) so that \(\mathcal{X} = \mathbb{R}\). Suppose \(\hat{\mu}\) is an estimate of the regression function obtained from the proper training set \(D_\mathrm{tr}\). In the case of split conformal prediction, we refer to the conformity score \[\hat{s}(x,y) = |y - \hat{\mu}(x)|\] as the \textit{absolute residual score}. Note that in this case the prediction set is an interval centered at \(\hat{\mu}(x) \) since \begin{align}
    C(x) &= \left\{y \in \mathbb{R}: \, |y - \hat{\mu}(x) | \leq \hat{Q}_{(\hat{S}_1, \ldots, \hat{S}_n, \infty)}(1-\alpha) \right\} \nonumber \\
    &= \left[ \hat{\mu}(X_{n+1}) - \hat{Q}_{(\hat{S}_1, \ldots, \hat{S}_n, \infty)}(1-\alpha), \ \hat{\mu}(x) + \hat{Q}_{(\hat{S}_1, \ldots, \hat{S}_n, \infty)}(1-\alpha) \right] \label{eqn:split_absolute_residual_prediction_set}.
\end{align}
\noindent
It is important to note that this simplified form of the prediction set is a consequence of \cref{lemma:quantile_lemma} and split conformal prediction procedure. Specifically, this form of the prediction set arises from the fact that neither the estimated regression function, nor the quantile used in the definition of the prediction set depend on \(y\). The former is a consequence of the split conformal prediction procedure, which ensures \(\hat{\mu}\) only depends on the proper training set \(D_\mathrm{tr}\). The latter is a consequence of both the split conformal algorithm, which ensures \(S_i^y = \hat{s}(X_i, Y_i)\) is independent of \(y\) (as in the proof of \cref{thm:splitconformal_coverage}), and \cref{lemma:quantile_lemma}, which ensures the quantile used in the prediction set depends only on \(D_\mathrm{cal}\).
\label{example:splitCP_absolute_residual_score}
\end{example}

\noindent
We now present a numerical experiment to provide a concrete example of split conformal prediction to illustrate the ideas from this subsection. As mentioned in the above remarks, both the form of the conformity score \(\hat{s}\) and the fitting procedure used to obtain it may, in theory, be arbitary. However, in practice, both of these can significantly affect the final prediction set. The choice of the form of the conformity score will be discussed in more detail in \cref{subsec:conformityscore}. In the numerical experiment below, we demonstrate the effect of the choice of the fitting procedure on the prediction set. \vskip5pt

\noindent
We conduct a numerical experiment with simulated data generated as follows: \begin{align*}
    X_1, X_2, \ldots &\overset{\R{i.i.d.}}{\sim} \R{Uniform}(-5, 5) \\
    Y_i | X_i &\overset{\R{indep.}}{\sim} \mathcal{N}\left( \mu(X_i), \ 0.5^2 \right),
\end{align*} for each \(i\), where \[\mu(x) = \frac{1}{1+x^2} + \frac{2}{1+(x-3)^2}.\] We use this data-generating process to generate independent proper training, calibration and test datasets with sizes \(1000\), \(3000\) and \(3000\), respectively. We train regression models on the proper training set using both linear regression and random forests, where hyperparameters of the random forest are chosen via \(5-\)fold cross-validation. We then use split conformal prediction with the absolute residual score on the calibration dataset to prediction sets for each data point in the test dataset. \textcolor{red}{The code for this numerical experiment is provided in [insert filename].} We now compare the prediction intervals obtained using both of these models.\vskip5pt

\noindent
Given a test dataset \(D_\R{test}\) with i.i.d data points, as above, we may compute the \textit{empirical coverage} \[\frac{1}{|D_\R{test}|} \sum_{(X, Y) \in D_\R{test}} \mathbbm{1}\{ Y \in C(X) \}\] as an estimate of the true coverage. By \cref{thm:splitconformal_coverage}, we expect the empirical coverage to be close to \(1-\alpha\) for any choice of fitting procedure and conformity score, provided \(|D_\R{test}|\) is large. Indeed, for this numerical example, we find that the empirical coverage is \(0.8993\) and \(0.9053\) for the linear regression and random forest model, respectively. Although both methods provide the desired coverage, it is clear from \cref{fig:split_LR_RF} that the random forests model is a better fit compared to the linear regression model. This is reflected by the fact that the average length of the conformal prediction intervals in the random forest model (1.6978) is smaller than that in the linear regression model (2.0773). Mathematically, we can explain this by considering \eqref{eqn:split_absolute_residual_prediction_set}. Since the distribution of the absolute residual scores of the calibration data points is more skewed towards zero (due to the random forests model having a better fit), the quantile \(\hat{Q}_{\hat{S}} \left( \frac{\lceil (1-\alpha)(n+1) \rceil}{n} \right)\) will be lower for the random forests model and so the prediction intervals will be shorter on average. Overall, this numerical experiment highlights that whilst any model fitting procedure can theoretically be used, a better fitting procedure is more desirable as the resulting conformal prediction intervals are on average narrower.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{figures/2_3_LR_RF_example.png}
    \caption{Plot of the test data, the fitted regression function and the split conformal prediction interval using linear regression (left) and random forests (right). The fitted regression function is denoted \(\hat{\mu}_{LR}\)} (left) and \(\hat{\mu}_{RF}\) (right). \label{fig:split_LR_RF}
\end{figure}

\subsubsection{Training conditional coverage}

Note that the coverage guarantee in \cref{thm:splitconformal_coverage} is marginal over the calibration data \(D_\R{cal}\), i.e. the coverage guarantee is equivalent to \[\Prob{Y_{n+1} \in C(X_{n+1})} = \Exp{\Ind{Y_{n+1} \in C(X_{n+1})} \, | \, D_\R{cal}}{[}{]} \geq 1-\alpha. \] This means that for a randomly drawn calibration set and test point, the prediction set contains the test point with probability \(1-\alpha\).  However, in practice, it may often be the case that only one calibration set is available, and we wish to understand the probability of a random test point being covered conditional on this calibration set, i.e. the quantity \[\Prob{Y_{n+1} \in C(X_{n+1}) \, | \, D_\R{cal}}.\] Note that this is a random quantity since it is a function of the calibration data. In the case of split conformal prediction, it is possible to exactly derive the distribution of this quantity. We provide our own proof of this result, following the steps outlined in \cite{tibs_advanced_topics_homework}.

\begin{lemma}[\cite{tibs_advanced_topics}]
    Suppose \((X_1, Y_1), \ldots, (X_{n+1}, Y_{n+1})\) are i.i.d. and suppose that \(\hat{S}_i\) has a continuous distribution. Then \begin{equation}
        \Prob{Y_{n+1} \in C(X_{n+1}) \, | \, D_\R{cal}} \sim \R{Beta}(k_\alpha, n + 1 - k_\alpha), 
    \label{eqn:beta_training_conditional}
    \end{equation} where \(k_\alpha = \lceil (1-\alpha)(n+1) \rceil\).
\end{lemma}
\begin{proof}
    We first claim that if \(U_1, \ldots, U_n \overset{\R{i.i.d.}}{\sim} \R{Uniform}(0,1)\) with order statistics \(U_{(1)} \leq \cdots \leq U_{(n)}, \) then for any \(k \in [n]\), \[U_{(k)} \sim \R{Beta}(k, n+1-k).\] Note that for any \(x \in \mathbb{R}\), we have that \[\Prob{U_{(k)} \leq x} = \sum_{r=k}^{n} \binom{n}{r} x^r (1-x)^{n-r}\] since at least \(k\) of the random variables \(U_1, \ldots, U_n\) must be less than or equal to \(x\) and \(\Prob{U_i \leq x} = x\) for any \(i \in [n]\). Therefore, if \(g(z) = z^r (1-z)^{n-r}\)we have that \begin{align*}
        \lim_{\epsilon \to 0} \frac{\Prob{x \leq U_{(k)} \leq x+\epsilon}}{\epsilon} &= \sum_{r=k}^{n} \lim_{\epsilon \to 0} \frac{g(x+\epsilon) - g(x)}{\epsilon} \\
        &= \sum_{r=k}^{n} \binom{n}{r} g'(x) \\
        &= \sum_{r=k}^{n} \binom{n}{r} r x^{r-1}(1-x)^{n-r} - \sum_{r=k}^{n} \binom{n}{r} (n-r) x^{r}(1-x)^{n-r-1} \\
        &= n \sum_{r=k-1}^{n-1} \binom{n-1}{r} r x^{r-1}(1-x)^{n-r} -  n \sum_{r=k}^{n-1} \binom{n-1}{r} (n-r) x^{r}(1-x)^{n-r-1} \\
        &= n \binom{n-1}{k-1} x^{k-1}(1-x)^{n-k},
    \end{align*}
    which is the density of a \(\R{Beta}(k, n+1-k)\) distribution. 
\end{proof}

\noindent
Let \(F\) be the cumulative distribution function of \(\hat{S}_1, \ldots, \hat{S}_{n+1}.\) It is a standard result (\textcolor{red}{maybe include quick proof}) that \(F(\hat{S}_1), \ldots, \hat{F}(S_{n+1}) \overset{\R{i.i.d.}}{\sim} \R{Uniform}(0,1).\) Therefore, for any \(k \in [n]\) \begin{align*}
    \Prob{\hat{S}_{n+1} \leq \hat{S}_{(k)} \, | \, D_\R{cal}} = F(\hat{S}_{(k)}) \overset{\R{d}}{=} U_{(k)} \sim \R{Beta}(k, n+1-k)
\end{align*} since \(F\) is increasing. Finally, we note that \[\Prob{Y_{n+1} \in C(X_{n+1}) \, | \, D_\R{cal}} = \Prob{\hat{S}_{n+1} \leq \hat{S}_{(k_\alpha)} \, | \, D_\R{cal}}\] by \textcolor{red}{remark to be added}, and so the result follows.


\subsection{Choice of conformity score}
\label{subsec:conformityscore}

In \cref{subsec:splitconformal}, we showed that any conformity score can be used to construct the split conformal prediction set \eqref{eqn:split_absolute_residual_prediction_set}. However, it is not immediately clear from \eqref{eqn:split_absolute_residual_prediction_set} how the choice of conformity score affects the prediction set. We gain some insight into this from \cref{example:splitCP_absolute_residual_score}, where we see that for split conformal prediction with the absolute residual score, the conformity score influences the width of the prediction interval \textcolor{red}{through the quantile}. In this subsection, we further explore how the choice of conformity score affects the properties of the resulting prediction set. In addition to the absolute residual score, we consider two further examples of conformity scores in the regression setting and compare them through numerical experiments. Throughout this subsection, we use split conformal prediction.

\subsubsection{Regression}

We work in the regression setting as in \cref{example:splitCP_absolute_residual_score} with \(\mathcal{X} = \mathbb{R}\). Consider the absolute residual score and its corresponding predicition interval \eqref{eqn:split_absolute_residual_prediction_set}. We observe that a consequence of using the absolute residual score is that the prediction interval has a constant width for all \(x \in \mathbb{R}.\) If the data generating process is heteroscedastic, i.e. \(\R{Var}(Y | X = x)\) is not constant in \(x\), then the prediction interval \eqref{eqn:split_absolute_residual_prediction_set} does not accurately capture the uncertainty in \(Y\) given \(X = x\). This is closely related to the fact that \cref{thm:splitconformal_coverage} only guarantees \textit{marginal coverage} \[\Prob{Y_{n+1} \in C(X_{n+1}) } \geq 1-\alpha\] as opposed to \textit{test-conditional coverage} \[\Prob{Y_{n+1} \in C(X_{n+1}) \, | \, X_{n+1} } \geq 1-\alpha.\] This means that conformal prediction does not guarantee \(1-\alpha\) level coverage at every \(x \in \mathcal{X}\), but rather only on average over \(x\). The two conformity scores we present aim to make the prediction interval adaptive to heteroscedasticity. 

\subsubsection{Locally Weighted Residual Score}

If \(\hat{\mu}\) is an estimate of the regression function \(\mu:x \mapsto \Exp{Y | X = x}{(}{)}\) and \(\hat{\sigma}\) is an estimate of the \textit{conditional mean absolute deviation} \(x \mapsto \Exp{|Y-\mu(X)| \, \mid X = x}{(}{)}\), then the \textit{locally weighted score} is the conformity score given by \begin{equation}
    \hat{s}(x,y) = \frac{| y - \hat{\mu}(x)|}{\hat{\sigma}(x)}.
\end{equation} The corresponding prediction set is given by \begin{equation}
    C(x) = \left[ \hat{\mu}(x) - \hat{\sigma}(x) \, \hat{Q}_{(\hat{S}_1, \ldots, \hat{S}_n, \infty)}(1-\alpha), \ \hat{\mu}(x) + \hat{\sigma}(x) \, \hat{Q}_{(\hat{S}_1, \ldots, \hat{S}_n, \infty)}(1-\alpha) \right],
\end{equation} using the notation of \cref{subsec:splitconformal}. \vskip5pt

\noindent
This conformity score was originally introduced in \cite{lei2018} and aims to account for heteroscedasticity but scaling the width of the interval in \eqref{eqn:split_absolute_residual_prediction_set} by \(\hat{\sigma}(x)\) for each \(x \in \mathcal{X}\). In practice, \(\hat{\sigma}\) can be estimated by first regressing \(Y_i\) onto \(X_i\) for \((X_i, Y_i) \in D_\R{tr}\) to obtain \(\hat{\mu}\) and then regress \(|Y_i -\hat{\mu}(X_i)|\) onto \(X_i\) for \((X_i, Y_i) \in D_\R{tr}\).

\subsubsection{Conformalised Quantile Regression}

A second approach to generate prediction intervals that are adaptive to heteroscedasticity is to estimate the conditional quantile function \begin{equation}
    q_{\tau}(x) = \inf \left\{ z \in \mathbb{R}: \Prob{Y \leq z | X = x} \geq \tau \right\}, \quad \tau \in (0,1)
\end{equation} directly. This is motivated by noting that \[\Prob{Y \in [q_{\alpha/2}(X), q_{1-\alpha/2}(X)] \ | \  X} = 1-\alpha,\] i.e. the interval \([q_{\alpha/2}(X), q_{1-\alpha/2}(X)]\) has exact \((1-\alpha)-\)level test-conditional coverage. The approach of estimating \(q_\tau(x)\) is referred to as \textit{quantile regression}. In this essay, we do not discuss details of the numerous methods for constructing quantile regression estimators. However, we note that the following important fact. \vskip5pt

\noindent
Define the \(\tau-\)\textit{pinball loss} by \(\ell(y, y') = \rho_\tau(y - y')\), where \[\rho_\tau(u) = u(\tau - \Ind{u < 0} ) = \begin{cases}
    u\tau &\quad \text{if } u \geq 0, \\
    u(\tau - 1) &\quad \text{otherwise.}
\end{cases}\]

\begin{lemma}
    Let \(U\) be a real-valued random variable with density \(f\) and strictly increasing cumulative distribution function \(F\). Then for all \(\tau \in (0,1)\), we have that \[F^{-1}(\tau) = \argmin_{t \in \mathbb{R}} \Exp{\ell(Y,t)}{(}{)}\] 
\end{lemma}
\begin{proof}
    We have that \begin{align*}
        \Exp{\ell(Y,t)}{(}{)} = \Exp{\rho_\tau(Y-t)}{(}{)} &= \int_{-\infty}^\infty \rho_\tau(y-t) f(y) \, \R{d}y \\
        &= \int_{-\infty}^t (\tau-1) (y-t) f(y) \, \R{d}y + \int_t^\infty \tau(y-t) f(y) \, \R{d}y.
    \end{align*}
    Equating the derivative of the above expression with respect to \(t\) to \(0\) gives \[\int_{-\infty}^t (1-\tau) f(y) \, \R{d}y - \int_t^\infty \tau f(y) \, \R{d}y = (1-\tau)F(t) - \tau(1-F(t))  =  0,\] which is equivalent to \[t = F^{-1}(\tau).\] The second derivative is equal to \(f(t)\), so \(F^{-1}(\tau)\) is indeed a minimiser.
\end{proof}

\noindent
Therefore, in the same way that training a model with respect to the least-squares loss gives an estimate for the conditional mean, training a model with respect to the \(\tau-\)pinball loss gives an estimate of the \(\tau^{\R{th}}\) quantile of the conditional distribution \(Y|X\). \vskip5pt

\noindent
Using a quantile regression procedure, we may obtain estimates \(\hat{q}_{\alpha/2}(x)\) and \(\hat{q}_{1-\alpha/2}(x)\) for \(q_{\alpha/2}(x)\) and \(q_{1-\alpha/2}(x)\), respectively. However, the interval \([\hat{q}_{\alpha/2}(x), \hat{q}_{1-\alpha/2}(x)]\) may not be guaranteed to have \((1-\alpha)-\)level coverage in finite samples. \textit{Conformalised quantile regression} \cite{romano2019_CQR} calibrates this interval using conformal prediction to provide it with a finite-sample coverage guarantee as in \cref{thm:splitconformal_coverage}. \vskip5pt

\noindent
After obtaining estimates \(\hat{q}_{\alpha/2}(x)\) and \(\hat{q}_{1-\alpha/2}(x)\) from the proper training set, conformalised quantile regression applies split conformal prediction with the conformity score \begin{equation}
    \hat{s}(x,y) = \max\left\{ \hat{q}_{\alpha/2}(x) - y, \, y - \hat{q}_{1-\alpha/2}(x) \right\}.
\label{eqn:CQR_conformity_score}
\end{equation} This results in the conformal prediction interval \begin{align}
    &\left\{ y \in \mathbb{R}: \hat{q}_{\alpha/2}(X_{n+1}) - y \leq \hat{Q}_{(\hat{S}_1, \ldots, \hat{S}_n, \infty)}(1-\alpha) \quad \R{and} \quad y - \hat{q}_{1-\alpha/2}(X_{n+1}) \leq \hat{Q}_{(\hat{S}_1, \ldots, \hat{S}_n, \infty)}(1-\alpha) \right\} \nonumber \\
    &= \left[\hat{q}_{\alpha/2}(X_{n+1}) - \hat{Q}_{(\hat{S}_1, \ldots, \hat{S}_n, \infty)}(1-\alpha), \hat{q}_{1-\alpha/2}(X_{n+1}) + \hat{Q}_{(\hat{S}_1, \ldots, \hat{S}_n, \infty)}(1-\alpha)  \right].
\end{align}

\noindent
To intuitively understand this score, we note the following. If \(y < \hat{q}_{\alpha/2}(x)\), i.e. \(y\) is below the predicted lower quantile, then \(\hat{s}(x,y) = |y - \hat{q}_{\alpha/2}(x)|\) is the absolute error compared to the predicted lower quantile. Similarly, if \(y > \hat{q}_{1-\alpha/2}(x)\), then \(\hat{s}(x,y) = |y - \hat{q}_{1-\alpha/2}(x)|.\) In both of these cases, \(\hat{s}(x,y) \geq 0\), indicating the interval \([\hat{q}_{\alpha/2}(x), \hat{q}_{1-\alpha/2}(x)]\) has failed to cover \(y\), and its value measures the magnitude of the error in the fitted model with respect to \((x,y)\). If \(\hat{q}_{\alpha/2}(x) < y < \hat{q}_{1-\alpha/2}(x)\), i.e. the interval \([\hat{q}_{\alpha/2}(x), \hat{q}_{1-\alpha/2}(x)]\) covers \(y\), then \(\hat{s}(x,y) < 0\) and \[\hat{s}(x,y) = \min \left\{ y - \hat{q}_{\alpha/2}(x), \, \hat{q}_{1-\alpha/2}(x) - y \right\},\] which may be interpreted as the smaller of the two "margins" by which the interval \([\hat{q}_{\alpha/2}(x), \hat{q}_{1-\alpha/2}(x)]\) covers \(y\). Note that this contrasts to the conformity scores we have seen previously, which could only take on non-negative values. We see from the above that if the estimated interval \([\hat{q}_{\alpha/2}(x), \hat{q}_{1-\alpha/2}(x)]\) overcovers, then conformalised quantile regression narrows the interval, and if the estimated interval undercovers, then conformalised quantile regression widens the interval.

\subsubsection{Numerical Experiments}

We now present numerical experiments designed to highlight that the locally weighted score and conformalised quantile regression are more adaptive to heteroscedasticity. We consider two data generating processes. Setting 1 generates i.i.d. data points with homoscedastic noise, and setting 2 generates i.i.d data points with heteroscedastic noise.
\begin{enumerate}[(i)]
    \item \textbf{Setting 1: } \begin{align*}
        X_1, X_2, \ldots &\overset{\R{i.i.d}}{\sim} \R{Uniform}(-5,5) \\
        \epsilon_1, \epsilon_2, \ldots &\overset{\R{i.i.d}}{\sim} \mathcal{N}(0,1) \\
        Y_i &= 1 - X_i + 2\epsilon_i. 
    \end{align*} for all \(i \in [n]\).
    \item \textbf{Setting 2: } \begin{align*}
        X_1, X_2, \ldots &\overset{\R{i.i.d}}{\sim} \R{Uniform}(-5,5) \\
        \epsilon_1, \epsilon_2, \ldots &\overset{\R{i.i.d}}{\sim} \mathcal{N}(0,1) \\
        Y_i &= 1 - X_i + \frac{1}{2}(|X_i| + 2)(\sin(2X_i) + 3/2) \, \epsilon_i
    \end{align*} for all \(i \in [n]\).    
\end{enumerate}

\noindent
In each setting, we generate independent proper training, calibration and test datasets, set \(\alpha = 0.1\) and generate conformal prediction intervals for the test dataset using the absolute residual score, the locally weighted residual score and conformalised quantile regression. \textcolor{red}{Further details on the exact implementation and the code are provided in the appendix.} In \cref{tab:2_4_settings1_2_results}, we record the empirical coverage and the average length of the prediction intervals calculated on the test dataset. In \cref{fig:2_4_homoscedastic} and \cref{fig:2_4_heteroscedastic}, we plot the prediction intervals obtained using these three methods. \vskip5pt

\noindent
We observe that all three methods provide the target coverage in both settings. This is guaranteed by the theoretical coverage guarantee for split conformal prediction in \cref{thm:splitconformal_coverage} since all the data points are i.i.d., and thus exchangeable. However, we clearly see in \cref{fig:2_4_heteroscedastic} that the width of the prediction intervals \(C(x)\) obtained using the locally weighted residual score and conformalised quantile regression vary with \(x\) to account for the heteroscedasticity. Since the absolute residual score generates prediction intervals whose width is constant (in \(x\)), we see in \cref{fig:2_4_heteroscedastic_RF} that these overcover in some regions and undercover in others. Therefore, the absolute residual score yields wider prediction intervals as compared to the other two methods, which can be seen in \cref{tab:2_4_settings1_2_results}. This is further emphasised in \cref{fig:2_4_local_cov_setting2}, where we estimate the conditional coverage \(\Prob{Y_{n+1} \in C(X_{n+1} ) | X_{n+1}}\) by dividing the covariate space \((-5,5)\) into smaller subintervals and calculating the empirical coverage for the test dataset on each subinterval. This plot demonstrates that the locally weighted score and conformalised quantile regression provide improved conditional coverage compared to the absolute residual score.

\begin{table}[h]
    \centering
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{|c|c|p{1.5cm}|c|p{1.5cm}|c|c|}
        \hline
        & \multicolumn{2}{c|}{Absolute residual} & \multicolumn{2}{c|}{Locally weighted} & \multicolumn{2}{c|}{Conformalised quantile regression} \\
        \hline
        & Coverage & Average length & Coverage & Average length & Coverage & Average length \\
        \hline
        Setting 1 & 0.9010 & 6.616 & 0.9027 & 6.632 & 0.9017 & 6.712 \\
        Setting 2 & 0.9003 & 13.03 & 0.9063 & 11.58 & 0.9027 & 11.55 \\
        \hline
    \end{tabular}
    \caption{Empirical coverage and average length of the conformal prediction intervals on the test dataset}
    \label{tab:2_4_settings1_2_results}
\end{table}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\linewidth]{figures/2_4_homoscedastic_RF.png}    
    \caption{Absolute residual score} \label{fig:2_4_homoscedastic_RF}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\linewidth]{figures/2_4_homoscedastic_LW.png}
    \caption{Locally weighted score} \label{fig:2_4_homoscedastic_LW}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\linewidth]{figures/2_4_homoscedastic_CQR.png}    
    \caption{Conformalised quantile regression} \label{fig:2_4_homoscedastic_CQR}
    \end{subfigure}
    \caption{Conformal prediction intervals in setting 1.}
\label{fig:2_4_homoscedastic}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\linewidth]{figures/2_4_heteroscedastic_RF.png}    
    \caption{Absolute residual score} \label{fig:2_4_heteroscedastic_RF}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\linewidth]{figures/2_4_heteroscedastic_LW.png}
    \caption{Locally weighted score} \label{fig:2_4_heteroscedastic_LW}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\linewidth]{figures/2_4_heteroscedastic_CQR.png}    
    \caption{Conformalised quantile regression} \label{fig:2_4_heteroscedastic_CQR}
    \end{subfigure}
    \medskip
    \begin{subfigure}{0.45\textwidth}
    \includegraphics[width=\linewidth]{figures/2_4_local_cov_setting2.png}    
    \caption{Local coverage} \label{fig:2_4_local_cov_setting2}
    \end{subfigure}
    \caption{Conformal prediction intervals in setting 2.}
\label{fig:2_4_heteroscedastic}
\end{figure}


\section{Extensions of Conformal Prediction}
\label{sec:extensions_of_CP}

In \cref{sec:foundations_of_CP}, we saw that the coverage guarantee of full conformal prediction relied on two key assumptions: the exchangeability of the data and the symmetry of the conformity score. \textcolor{red}{We saw that if the exchangeability assumption is violated, the coverage may deviate significantly from the target \(1-\alpha\) level.} \textcolor{red}{Something about why it is important to generalise to nonexchangeable data and examples (e.g. distribution shift) and non-symmetric conformity scores (e.g. can use algorithm that does not treat data symmetrically such as WLS.)} 

\subsection{Nonexchangeable Conformal Prediction}
\label{subsec:nex_CP}

In this subsection, we present the \textit{nonexchangeable conformal prediction} (NexCP) method developed in \cite{barber2023conformalbeyondexch}. The key theoretical contribution of \cite{barber2023conformalbeyondexch}, presented in \cref{thm:nexCP_coverage}, is the development of a weighted conformal prediction procedure for which a coverage guarantee can be derived when the data is not assumed to be exchangeable and the conformity score is not assumed to be symmetric. \vskip5pt

\noindent
Before we present the main theorem of this subsection, we introduce the required notation. Given data points \((X_1, Y_1), \ldots, (X_{n+1}, Y_{n+1}) \in \mathcal{Z}\), \(y \in \mathcal{Y}\) and \(k \in [n+1]\), we define \[D = ((X_i, Y_i))_{i=1}^{n+1}, \quad D^{y} = ((X_i, Y_i)_{i=1}^n, (X_{n+1}, y)), \quad \quad D^{(k)} = \pi_k(D), \quad \mathrm{and} \quad D^{y, (k)} = \pi_k(D^{y}). \] where \(\pi_k \in S_{n+1}\) is the transposition exchanging \(k\) and \(n+1\). \vskip5pt

\noindent
Let \(s\) be  conformity score. As mentioned above, we will not assume that \(s\) is symmetric. This means that the model fitting procedure within the conformity score may take the order of the data points into account.  We define \[S = (s((X_i, Y_i); D))_{i=1}^{n+1}, \quad \mathrm{and} \quad S^{(k)} = ( s(( X_{\pi_k(i)}, Y_{\pi_k(i)} ); D^{(k)}) )_{i=1}^{n+1}.\] We also define \[S_i^{y, (k)} = \begin{cases}
    s(( X_i, Y_i ); D^{y, (k)}) \quad &\mathrm{if} \ i = 1, \ldots, n \\
    s(( X_{n+1}, y ); D^{y, (k)}) \quad &\mathrm{if} \ i = n+1,
\end{cases} \quad \mathrm{and} \quad S^{y, (k)} = (S_i^{y,(k)})_{i=1}^{n+1}.\]

\noindent
The NexCP method assigns \textit{weights} \(w_1, \ldots, w_{n} \in [0, \infty)\) to the data points \((X_1, Y_1), \ldots, (X_n, Y_n)\), respectively. The weights are fixed, non-negative real numbers and the corresponding normalised weights are defined by \begin{equation}
    \tilde{w_i} = \frac{w_i}{1+\sum_{j=1}^n w_j} \quad \mathrm{and} \quad \tilde{w}_{n+1} = \frac{1}{1+\sum_{j=1}^n w_j}
\label{eqn:nexCP_weights_defn}
\end{equation} for all \(i \in [n]\). \vskip5pt

\noindent
We define \(K\) to be random variable taking values in \([n+1]\) such that \begin{equation}
    \Prob{K = k} = \tilde{w}_k
\label{eqn:nexCP_K_defn}
\end{equation} for all \(k \in [n+1]\), and we take \(K\) and \(D\) to be independent. \vskip5pt

\noindent
Lastly, we recall the definition of the total variation distance.

\begin{definition}
    Let \(U\) and \(V\) be random variables defined on the probability space \((\Omega, \mathcal{F}, \mathbb{P})\). The \textit{total variation distance} between \(U\) and \(V\) is defined as \[\R{d_{TV}}(U,V) = \sup_{A \in \mathcal{F}} | \Prob{U \in A} - \Prob{V \in A}| .\] 
\label{defn:TV_distance}
\end{definition}

\noindent
We now prove the main theorem of this subsection which provides the predicition set and coverage guarantee for the NexCP method.

\begin{theorem}
    Let \((X_1, Y_1), \ldots, (X_{n+1}, Y_{n+1}) \in \mathcal{Z}\) be a sequence of data points and \(s\) be a conformity score. Let \(w_1, \ldots, w_{n+1} \in [0, \infty)\) be fixed real numbers and define \(\tilde{w}_i\) according to \eqref{eqn:nexCP_weights_defn} for all \(i \in [n+1]\). Let \(K\) be a random variable as in \eqref{eqn:nexCP_K_defn} that is independent of \(D\). Define the prediction set \begin{equation}
        C(X_{n+1}) = \left\{ y \in \mathcal{Y} : \ S_{n+1}^{y, (K)} \leq \hat{Q}_{S^{y, (K)}}^{\tilde{w}}(1-\alpha)  \right\}.
    \label{eqn:nexCP_prediction_set}
    \end{equation} Then we have that \begin{equation}
        \Prob{Y_{n+1} \in C(X_{n+1})}  \geq 1-\alpha - \sum_{k=1}^n \tilde{w}_k \R{d_{TV}}(S, S^{(k)}).
    \end{equation}
\label{thm:nexCP_coverage}
\end{theorem}
\begin{proof}
    If \(Y_{n+1} \not \in C(X_{n+1})\), then \[S_{n+1}^{Y_{n+1}, (K)} > \hat{Q}^{\tilde{w}}_{S_{n+1}^{Y_{n+1}, (K)}}(1-\alpha).\] This implies that \[S_{n+1}^{Y_{n+1}, (K)} > \hat{Q}^{\tilde{w}}_{\left( S_1^{Y_{n+1}, (K)}, \ldots, S_n^{Y_{n+1}, (K)}, \infty  \right)}(1-\alpha)\] by \cref{lemma:quantile_lemma}. We now claim that \[\hat{Q}^{\tilde{w}}_{\left( S_1^{Y_{n+1}, (K)}, \ldots, S_n^{Y_{n+1}, (K)}, \infty  \right)}(1-\alpha) \geq \hat{Q}^{\tilde{w}}_{S^{(K)}}(1-\alpha).\] If \(K = n+1\), this is shown in the proof of \cref{lemma:quantile_lemma}. If \(K \leq n\), we have that for any \(x \in \mathbb{R}\), \begin{align*}
        \hat{F}^{\tilde{w}}_{S^{(K)}}(x) &= \sum_{i=1}^{n+1} \tilde{w}_i \Ind{x \geq S_{\pi_K(i)}^{Y_{n+1}, (K)}} \\
        &= \sum_{\substack{i = 1 \\ i \neq K}}^{n} \tilde{w}_i \Ind{x \geq S_i^{Y_{n+1}, (K)}} + \tilde{w}_K \Ind{x \geq S_{n+1}^{Y_{n+1}, (K)}} + \tilde{w}_{n+1} \Ind{x \geq S_K^{Y_{n+1}, (K)}} \\
        &= \hat{F}^{\tilde{w}}_{\left( S_1^{Y_{n+1}, (K)}, \ldots, S_n^{Y_{n+1}, (K)}, \infty  \right)}(x) + \tilde{w}_K \Ind{x \geq S_{n+1}^{Y_{n+1}, (K)}} + \tilde{w}_{n+1} \Ind{x \geq S_K^{Y_{n+1}, (K)}} \\
        & \hspace{4.8cm} -  \tilde{w}_K \Ind{x \geq S_{K}^{Y_{n+1}, (K)}} - \tilde{w}_{n+1} \Ind{x \geq \infty} \\
        &= \hat{F}^{\tilde{w}}_{\left( S_1^{Y_{n+1}, (K)}, \ldots, S_n^{Y_{n+1}, (K)}, \infty  \right)}(x) + \tilde{w}_K \left( \Ind{x \geq S_{n+1}^{Y_{n+1}, (K)}} - \Ind{x \geq \infty} \right) \\
        & \hspace{4.8cm} + \left( \tilde{w}_{n+1} - \tilde{w}_K\right) \left(\Ind{x \geq S_{K}^{Y_{n+1}, (K)}} - \Ind{x \geq \infty} \right) \\
        & \geq \hat{F}^{\tilde{w}}_{\left( S_1^{Y_{n+1}, (K)}, \ldots, S_n^{Y_{n+1}, (K)}, \infty  \right)}(x).
    \end{align*}
    Therefore, we have that \[\hat{F}^{\tilde{w}}_{S^{(K)}}\left( \hat{Q}^{\tilde{w}}_{\left( S_1^{Y_{n+1}, (K)}, \ldots, S_n^{Y_{n+1}, (K)}, \infty  \right)}(1-\alpha) \right) \geq 1-\alpha,\] by \cref{lemma:cdfquantile} which shows the claim above. \vskip5pt

    \noindent
    So far, we have shown that \[Y_{n+1} \not \in C(X_{n+1}) \implies S_{n+1}^{Y_{n+1}, (K)} >\hat{Q}^{\tilde{w}}_{S^{(K)}}(1-\alpha).\] Noting that \(S_{n+1}^{Y_{n+1}, (K)} = S_K^{(K)}\), this implies that \[\Prob{Y_{n+1} \in C(X_{n+1}) } \geq \Prob{ S_K^{(K)} \leq \hat{Q}^{\tilde{w}}_{S^{(K)}}(1-\alpha) }. \]
    \noindent
    Thus, we have that
    \begin{align*}
        \Prob{Y_{n+1} \in C(X_{n+1})} &\geq \Prob{ S_K^{(K)} \leq \hat{Q}^{\tilde{w}}_{S^{(K)}}(1-\alpha) } \\
        &= \sum_{k=1}^{n+1} \Prob{S_k^{(k)} \leq \hat{Q}^{\tilde{w}}_{S^{(k)}}(1-\alpha), \, K=k} \\
        &= \sum_{k=1}^{n+1} \tilde{w}_k \Prob{S_k^{(k)} \leq \hat{Q}^{\tilde{w}}_{S^{(k)}}(1-\alpha)} \\
        &= \sum_{k=1}^{n+1} \tilde{w}_k \Prob{S_k \leq \hat{Q}^{\tilde{w}}_{S}(1-\alpha)} \\
        & \hspace{2.6cm} + \sum_{k=1}^{n+1} \tilde{w}_k \left[ \Prob{S_k^{(k)} \leq \hat{Q}^{\tilde{w}}_{S^{(k)}}(1-\alpha)} - \Prob{S_k \leq \hat{Q}^{\tilde{w}}_{S}(1-\alpha)} \right] \\
        &\geq \Exp{\sum_{k=1}^{n+1} \tilde{w}_k \Ind{S_k \leq \hat{Q}^{\tilde{w}}_{S}(1-\alpha)}}{[}{]} - \sum_{k=1}^{n+1} \tilde{w}_k \, \R{d_{TV}}(S, S^{(k)}) \\
        &= \Exp{\hat{F}^{\tilde{w}}_S(\hat{Q}^{\tilde{w}}_S (1-\alpha))}{[}{]} - \sum_{k=1}^{n+1} \tilde{w}_k \, \R{d_{TV}}(S, S^{(k)}) \\
        &\geq 1-\alpha - \sum_{k=1}^{n+1} \tilde{w}_k \, \R{d_{TV}}(S, S^{(k)}),
    \end{align*}
    where the third line follows from the independence of \(K\) and \(D\), the fifth line follows from \cref{defn:TV_distance} and the final inequality follows from \cref{lemma:cdfquantile}.
\end{proof}

\noindent
The corresponding algorithm, referred to as \textit{nonexchangeable full conformal predicition} is stated below.

\begin{algorithm}[H]
    \label{alg:nexCP}
    \caption{Nonexchangeable full conformal prediction algorithm}
    \begin{algorithmic}
        \Require Calibration \(((X_i, Y_i))_{i=1}^n\); test predictor \(X_{n+1}\); miscoverage level \(\alpha \in (0,1)\), conformity score \(\hat{s}\). 
        \State Initialise \(C \gets \emptyset\)
        \State Draw \(K\) from \([n+1]\) according to \eqref{eqn:nexCP_K_defn}.

        \For{\(y \in \mathcal{Y}\)}
            \State Compute \(S_i^{y, (K)}\) for \(i = 1, \ldots, n+1\)
            \State Compute \(\hat{Q}^{\tilde{w}}_{S^{y, (K)}}\)
            \If{\(S_{n+1}^{y, (K)} \leq \hat{Q}^{\tilde{w}}_{S^{y, (K)}}\)}
                \State \(C \gets C \cup \{y\}\)
            \EndIf
        \EndFor
        \Ensure \(C\)
    \end{algorithmic}
\end{algorithm}

\noindent
Before discussing the implications of \cref{thm:nexCP_coverage}, we note that, in the same way that split conformal prediction is a special case of full conformal prediction in \cref{sec:foundations_of_CP}, we may derive a corresponding result for NexCP, referred to as \textit{nonexchangeable split conformal predicition}. We use the notation from \cref{subsec:splitconformal}, denoting \(\hat{s}:\mathcal{X} \to \mathcal{Y}\) to be the conformity score and \(D_\R{tr}\) the proper training set. We also write \(\hat{S}_i = \hat{s}(X_i, Y_i)\) for all \(i \in [n]\) and \(\hat{S}_{n+1}^y = \hat{s}(X_{n+1}, y)\).

\begin{corollary}
    Suppose \((X_1, Y_1), \ldots, (X_n, Y_n), (X_{n+1}, Y_{n+1}) \in \mathcal{Z}\) are data points. Define the prediction set \begin{equation}
        C(X_{n+1}) = \left\{ y \in \mathcal{Y}: \  \hat{S}_{n+1}^y \leq \hat{Q}^w_{(\hat{S}_1, \ldots, \hat{S}_n, \infty)}(1-\alpha)  \right\}.
    \label{eqn:nexCP_split_prediction_set}
    \end{equation} Then we have that \[\Prob{Y_{n+1} \in C(X_{n+1})}  \geq 1-\alpha - \sum_{k=1}^n \tilde{w}_k \R{d_{TV}}(S, S^{(k)}).\]
\label{corr:nexCP_split}
\end{corollary}
\begin{proof}
    This follows from \cref{thm:nexCP_coverage} and \cref{lemma:quantile_lemma} in exactly the same way as in the proof of \cref{thm:splitconformal_coverage} by taking \(s((x,y); \tilde{D}) = \hat{s}(x,y)\) for \((x,y) \in \mathcal{Z}\) and \(\tilde{D} = \cup_{j \geq 1} \mathcal{Z}^j.\)
\end{proof}

\noindent
We now interpret the result in \cref{thm:nexCP_coverage}, following the points made in \cite{barber2023conformalbeyondexch}. As mentioned at the beginning of this subsection, the coverage guarantee in \cref{thm:nexCP_coverage} makes no assumption on the distribution of the data or on the conformity score. The quantity \begin{equation}
    \sum_{k=1}^{n} \tilde{w}_k \R{d_{TV}}(S, S^{(k)}).
\label{eqn:nexCP_loss_coverage}
\end{equation} may be interpreted as the loss in coverage that occurs due to not assuming exchangeability or that the conformity score is symmetric. We highlight some important special cases of \cref{thm:nexCP_coverage}.

\begin{corollary}
    Let \((X_1, Y_1), \ldots, (X_{n+1}, Y_{n+1}) \in \mathcal{Z}\) and \(s\) be a symmetric conformity score. Let \(C(X_{n+1})\) be the prediction set \eqref{eqn:fullconformal_prediction_set}. Then we have that \[
        \Prob{Y_{n+1} \in C(X_{n+1})}  \geq 1-\alpha - \frac{1}{n+1} \sum_{k=1}^{n} \R{d_{TV}}(S, S^{(k)}).
    \]
\label{corr:nexCP_robustness}
\end{corollary}
\begin{proof}
    Since \(s\) is symmetric, we have that \(S_{i}^{y, (K)} = s((X_i, Y_i); D^{y, (K)}) = s((X_i, Y_i); D^{y}) = S_i^y\) for \(i \in [n]\) and similarly, \(S_{n+1}^{y, (K)} = S_{n+1}^y\). Moreover, if we take \(w_i = 1\) for each \(i \in [n]\), then the prediction set \eqref{eqn:nexCP_prediction_set} coincides with \eqref{eqn:fullconformal_prediction_set}, so the result follows from \cref{thm:nexCP_coverage}.
\end{proof}

\noindent
An interpretation of \cref{corr:nexCP_robustness} is that if we apply the standard full conformal prediction algorithm \cref{alg:fullconformal} to nonexchangeable data, then \[\frac{1}{n+1} \sum_{k=1}^{n} \R{d_{TV}}(S, S^{(k)})\] is the loss in marginal coverage compared to the \(1-\alpha\) level in the exchangeable case. If we interpret the quantity in the above display as a measure of the extent to which exchangeability is violated, then \cref{corr:nexCP_robustness} shows that smaller the violation of exchangeability, the smaller the loss in coverage.

\begin{corollary}
    Let \((X_1, Y_1), \ldots, (X_{n+1}, Y_{n+1}) \in \mathcal{Z}\) be exchangeable and \(s\) be a conformity score. With \(\tilde{w}\) and \(K\) as in \cref{thm:nexCP_coverage}, define the prediction set \[C(X_{n+1}) = \left\{ y \in \mathcal{Y} : \ S_{n+1}^{y, (K)} \leq \hat{Q}_{S^{y, (K)}}^{\tilde{w}}(1-\alpha)  \right\}.\] Then we have that \[
        \Prob{Y_{n+1} \in C(X_{n+1})}  \geq 1-\alpha.
    \]
\label{corr:nexCP_exchangeable}
\end{corollary}
\begin{proof}
    If the data is exchangeable, then \(S \overset{d}{=} S^{(k)}\) for any \(k \in [n] \), so \[\sum_{k=1}^n \tilde{w_k} \R{d_{TV}}(S, S^{(k)}) = 0,\] and the result follows from \cref{thm:nexCP_coverage}.
\end{proof}

\noindent
We see that \cref{corr:nexCP_exchangeable} is in fact a generalisation of \cref{thm:fullconformal_coverage} since it has the same assumptions and coverage guarantee as \cref{thm:fullconformal_coverage} but allows for conformity scores that are not necessarily symmetric. Therefore \cref{corr:nexCP_exchangeable} shows that in the special case that the data is exchangeable but the conformity score is not symmetric, the NexCP method may be used to recover the same \((1-\alpha)-\)level coverage guarantee as in \cref{thm:fullconformal_coverage}. \vskip5pt

\noindent
Overall, this shows that in addition to quantifying the coverage loss in the case where the data 

\subsection{Distribution Shift}
\label{subsec:dist_shift}

In this subsection, we focus on a specific kind of violation of exchangeability - distribution shift. Specifically, denoting \(Z_i = (X_i, Y_i) \in \mathcal{Z}\) for \(i \in [n+1]\), we will consider the case where the calibration data satisfies \(Z_1, \ldots, Z_n \overset{\R{i.i.d.}}{\sim} P\) from a distribution \(P\) and the test point \(Z_{n+1} \sim Q\) for some other distribution \(Q\). Note that \(Z_1, \ldots, Z_{n+1}\) are not exchangeable since they are not identically distributed, so \cref{thm:fullconformal_coverage} does not apply. We discuss a conformal predicition procedure originally developed in \cite{tibshirani2019covariateshift} and \cite{ramdas2021labelshift} which enables the construction of a prediction set with at least \(1-\alpha\) coverage in the above setting. \vskip5pt

\noindent
We first introduce a notion that generalises exchangeability, referred to as \textit{weighted exchangeability} (\cite{tibshirani2019covariateshift,barber2024finetti,tang2023finiteweighted}). In what follows \(\mathcal{U}\) denotes a separable complete metric space, \(\mathcal{B}(\mathcal{U})\) denotes the Borel \(\sigma-\)algebra on \(\mathcal{U}\) and \(\Lambda\) denotes the set of measurable functions from \(\mathcal{U}\) to \((0, \infty)\).  The conditions on \(\mathcal{U}\) are required due to measure-theoretic results ensuring the existence of regular conditional distributions which we do not discuss further here ((see Appendix A.1 \cite{barber2024finetti})). 

\begin{definition}[\cite{barber2024finetti,tang2023finiteweighted}]
    \begin{enumerate}[(i)] \itemsep0em
        \item A probability measure \(Q\) on \(\mathcal{U}^n\) is \textit{exchangeable} if for all \(A_1, \ldots, A_n \in \mathcal{B}(\mathcal{U})\), \[Q(A_1 \times \cdots \times A_n) = Q(A_{\sigma(1)} \times \cdots \times A_{\sigma(n)})\] for all \(\sigma \in S_n.\) 
        \item Given \(\lambda = (\lambda_1, \ldots, \lambda_n) \in \Lambda^n\), a probability measure \(Q\) on \(\mathcal{U}^n\) is called \(\lambda-\)\textit{weighted exchangeable} if the measure \(\bar{Q}\) defined as \[\bar{Q}(B) = \int_{B} \frac{\R{d}Q(x_1, \ldots, x_n)}{\lambda_1(x_1) \cdots \lambda_n(x_n)}, \quad \mathrm{for} \ B \in \mathcal{B}(\mathcal{U}^n) \] is exchangeable.
    \end{enumerate}
\label{defn:weighted_exch}
\end{definition}

\noindent
Note that \(U_1, \ldots, U_n\) are exchangeable according to \cref{defn:exch} if and only if \(\mathbb{P} \circ U^{-1}\) is exchangeable according to \cref{defn:weighted_exch}, where \(U = (U_1, \ldots, U_n)\). Moreover, note that \(Q\) is exchangeable if and only if \(Q\) is \(\lambda-\)weighted exchangeable for \(\lambda_1, \ldots, \lambda_n \equiv 1\). The lemma below demonstrates how the notion of weighted exchangeability applies to our setting.

\begin{lemma}
    Let \(Z_1, \ldots, Z_n \overset{\R{i.i.d}}{\sim} P \) be data points in \(\mathcal{Z}\) and suppose \(Z_{n+1} \sim Q\) is a data point in \(\mathcal{Z}\) independent of \((Z_i)_{i=1}^n\), for some distributions \(P, Q\) on \(\mathcal{Z}\) such that \(Q\) is absolutely continuous with respect to \(P\). Then the distribution of \((X_i, Y_i)_{i=1}^{n+1}\) is \(\lambda-\)weighted exchangeable, where \(\lambda = \left(1, \ldots, 1, \frac{\R{d}Q}{\R{d}P}\right)\) and \(\frac{\R{d}Q}{\R{d}P}\) is the Radon-Nikodym derivative.
\label{lemma:dist_shift_weighted_exch}
\end{lemma}
\begin{proof}
    Note that \((Z_i)_{i=1}^{n+1} \sim P^{n} \times Q\). For any measurable sets \(B_1, \ldots, B_{n+1}\) and \(\sigma \in S_{n+1}\), we have that \begin{align*}
        (\overline{P^n \times Q})(B_1 \times \cdots \times B_{n+1}) &= \int_{B_1 \times \cdots \times B_{n+1}} \frac{\R{d}(P^n \times Q)(u_1, \ldots, u_{n+1})}{\frac{\R{d}Q}{\R{d}P}(u_{n+1})} \\
        &= P(B_1) \cdots P(B_n) \int_{B_{n+1}} \frac{1}{\frac{\R{d}Q}{\R{d}P}(u_{n+1})} \, \R{d}Q(u_{n+1}) \\
        &= P(B_1) \cdots P(B_n) \int_{B_{n+1}} \frac{1}{\frac{\R{d}Q}{\R{d}P}(u_{n+1})} \, \frac{\R{d}Q}{\R{d}P}(u_{n+1}) \, \R{d}P(u_{n+1}) \\
        &= P^{n+1}(B_1 \times \cdots \times B_{n+1}).
    \end{align*}
    Therefore, we have that \begin{align*}
        (\overline{P^n \times Q})(B_1 \times \cdots \times B_{n+1}) &= P^{n+1}(B_1 \times \cdots \times B_{n+1}) \\
        &= P^{n+1}(B_{\sigma(1)} \times \cdots \times B_{\sigma(n+1)}) \\
        &= (\overline{P^n \times Q})(B_1 \times \cdots \times B_{n+1}) &= P^{n+1}(B_{\sigma(1)} \times \cdots \times B_{\sigma(n+1)}),
    \end{align*} so \(\overline{P^n \times Q}\) is exchangeable.
\end{proof}

\noindent
We now state an important lemma on weighted exchangeability that will be used to establish the desired coverage guarantee. The interpretation of this result, as mentioned in \cite{barber2024finetti}, is that conditional on the unordered values \(\{U_1, \ldots, U_k\}\), the distribution \(U_i\) is a discrete distribution over \(\{U_1, \ldots, U_k\}\) where the probabilities may be explicity expressed in terms of \(\lambda\). The probability of taking on the value \(U_j\) is denoted by \(\left(w_{k, i}(U_1, \ldots, U_k)\right)_j\) below. The way the authors of \cite{barber2024finetti} formally express conditioning on the unordered values \(\{U_1, \ldots, U_k\}\) is by conditioning on the sub-\(\sigma-\)algebra \(\mathcal{E}_k\) of \(\mathcal{B}(\mathcal{U}^k)\) satisfying \((u_1, \ldots, u_k) \in \mathcal{E}_k \iff (u_\sigma(k), \ldots, u_\sigma(k)) \in \mathcal{E}_k\) for any \(\sigma \in S_k\). We now state the lemma but omit the proof since it consists primarily of measure-theoretic calculations regarding conditional distributions. The proof may be found in \cite{barber2024finetti}. \textcolor{red}{Add reference to say that this is same as conditioning on empirical dist + defn of empirical dist.} \vskip5pt

\noindent

\begin{lemma}[\cite{barber2024finetti} Proposition 7]
    For any \(\lambda \in \Lambda^k\), any \(\lambda-\)weighted exchangeable \(Q\) on \(\mathcal{U}^k\), and \(U \sim Q\), we have that \[U_i | \hat{P}_k \sim \tilde{P}_{k, i}, \] where \(\tilde{P}_{k, i} = \sum_{j=1}^{k} \left(w_{k, i}(U_1, \ldots, U_k)\right)_j \, \delta_{U_j}\) and \[\left(w_{k, i}(u_1, \ldots, u_k)\right)_j = \frac{ \sum_{\sigma \in S_k : \, \sigma(i) = j} \lambda_1(u_{\sigma(1)}) \cdots \lambda_k(u_{\sigma(k)})  }{\sum_{\sigma \in S_k} \lambda_1(u_{\sigma(1)}) \cdots \lambda_k(u_{\sigma(k)})}\]
\label{lemma:weighted_exch_conditional_lemma}
\end{lemma}

\noindent
We now apply \cref{lemma:weighted_exch_conditional_lemma} to our setting by combining it with \cref{lemma:dist_shift_weighted_exch}.

\begin{lemma}[\cite{angelopoulos2024theoreticalfoundationsconformalprediction} Proposition 7.6]
    Let \(Z_1, \ldots, Z_n \overset{\R{i.i.d}}{\sim} P \) be data points in \(\mathcal{Z}\) and suppose \(Z_{n+1} \sim Q\) is a data point in \(\mathcal{Z}\) independent of \((Z_i)_{i=1}^n\), for some distributions \(P, Q\) on \(\mathcal{Z}\) such that \(Q\) is absolutely continuous with respect to \(P\). Then \[Z_{n+1} | \hat{P}_{n+1} \sim \sum_{i=1}^{n} w_i \, \delta_{Z_i},\] where \begin{equation}
        w_i = \frac{ \frac{\R{d}Q}{\R{d}P}(Z_i)  }{ \sum_{j=1}^{n+1}  \frac{\R{d}Q}{\R{d}P}(Z_j) }, \quad i \in [n+1].
    \label{eqn:dist_shift_weights}
    \end{equation}
\label{lemma:dist_shift_empirical_dist}
\end{lemma}
\begin{proof}
    By \cref{lemma:weighted_exch_conditional_lemma} we have that \[Z_{n+1} | \hat{P}_{n+1} \sim \tilde{P}_{n+1, n+1},\] where \(\tilde{P}_{n+1, n+1} = \sum_{i=1}^{k} \left(w_{n+1, n+1}(Z_1, \ldots, Z_{n+1})\right)_i \, \delta_{Z_i}\) and \begin{align*}
        w_{n+1, n+1}(z_1, \ldots, z_{n+1})_i &= 
        \frac{\sum_{\sigma \in S_{n+1}: \ \sigma(n+1) = i} \ \frac{\R{d}Q}{\R{d}P}(z_{\sigma(n+1)}) }{\sum_{\sigma \in S_{n+1}} \, \frac{\R{d}Q}{\R{d}P}(z_{\sigma(n+1)}) } \\
        &= \frac{n! \, \frac{\R{d}Q}{\R{d}P}(z_i)}{\sum_{j=1}^{n+1} \sum_{\sigma \in S_{n+1}: \ \sigma(n+1)=j} \ \frac{\R{d}Q}{\R{d}P}(z_j) } \\
        &= \frac{\frac{\R{d}Q}{\R{d}P}(z_i)}{\sum_{j=1}^{n} \frac{\R{d}Q}{\R{d}P}(z_j)}.
    \end{align*}
\end{proof}

\noindent
We now state the main theorem of this subsection which is the foundation of the weighted conformal prediction procedure for distribution shift.

\textcolor{red}{Some notation needs to be defined below.}

\begin{theorem}
    Let \(Z_1, \ldots, Z_{n+1} \overset{\R{i.i.d}}{\sim} P \) be data points in \(\mathcal{Z}\) and suppose \(Z_{n+1} \sim Q\) is a data point in \(\mathcal{Z}\) independent of \((Z_i)_{i=1}^n\), for some distributions \(P, Q\) on \(\mathcal{Z}\) such that \(Q\) is absolutely continuous with respect to \(P\). For \(y \in \mathcal{Y}\) and \(i \in [n]\), define \[w_i^y = \frac{\frac{\R{d}Q}{\R{d}P}(Z_i)}{\sum_{j=1}^n \frac{\R{d}Q}{\R{d}P}(Z_j) + \frac{\R{d}Q}{\R{d}P}(X_{n+1}, y)} \quad \R{and} \quad w_{n+1}^y = \frac{\frac{\R{d}Q}{\R{d}P}(X_{n+1}, y)}{\sum_{j=1}^{n} \frac{\R{d}Q}{\R{d}P}(Z_j) + \frac{\R{d}Q}{\R{d}P}(X_{n+1}, y)}.\] Define the prediction set \begin{equation}
        C(X_{n+1}) = \left\{y \in \mathcal{Y} : \, S_{n+1}^y \leq \hat{Q}^{w^y}_{S^y}(1-\alpha) \right\}.
    \label{eqn:dist_shift_prediction_set}
    \end{equation} Then we have that \[\Prob{Y_{n+1} \in C(X_{n+1})} \geq 1-\alpha.\]
\end{theorem}
\begin{proof}
    We have that \begin{align*}
        \Prob{Y_{n+1} \in C(X_{n+1})} = \Prob{S_{n+1} \leq Q^{w^{Y_{n+1}}}_S(1-\alpha)} = \Exp{ \Prob{S_{n+1} \leq Q^{w}_S(1-\alpha) \;\middle\vert\;  \hat{P}_{n+1} }}{[}{]}, 
    \end{align*} where \(w_i\) is as in \cref{eqn:dist_shift_weights} and we note that for all \(i \in [n+1]\), we have \(w_i^{Y_{n+1}} = w_i\). We first note that \[\sum_{i=1}^{n+1} w_i \delta_{S_i} = \frac{\sum_{i=1}^{n+1} \frac{\R{d}Q}{\R{d}P}(Z_i) \delta_{s(Z_i; D)}}{\sum_{i=1}^{n+1} \frac{\R{d}Q}{\R{d}P}(Z_i)}\] is invariant under permutations of \((Z_1, \ldots, Z_{n+1})\) since \(s\) is symmetric. Therefore, \(\hat{Q}^w_S (1-\alpha)\) is a function of \(\hat{P}_{n+1}\). Moreover, \cref{lemma:dist_shift_empirical_dist} and the symmetry of \(s\) imply that \[S_{n+1} \; | \; \hat{P}_{n+1} \sim \sum_{i=1}^{n+1} w_i \delta_{S_i}.\] Therefore, we have that \[\Prob{S_{n+1} \leq Q^{w}_S(1-\alpha) \;\middle\vert\;  \hat{P}_{n+1} } = \hat{F}^w_S(\hat{Q}^w_S(1-\alpha)) \geq 1-\alpha\] by \cref{lemma:cdfquantile}. The result follows upon taking expectations.
\end{proof}

%This is how you can start a new page
\vfill \eject

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                             
% REFERENCES
%                                                                             
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% List any references you have used below. You can list papers in your preferred style, e.g. as below.
% To cite a paper in your text, use \cite{}.

\nocite{*}
\printbibliography

% If you prefer, you can list your papers in a separate .bib file. This is often much more efficient and easier to change later.
% An explanation of how to do this can be found at https://www.overleaf.com/learn/latex/Bibliography_management_with_bibtex.
% When submitting the essay, you must upload your .bib file alongside this .tex file.

\end{document}